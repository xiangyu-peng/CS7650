{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab6ZUEZDPkyT"
   },
   "source": [
    "# Question Answering with SQUAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QE1iKRlYJey5"
   },
   "source": [
    "### Setup (if using Google Colaboratory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter_nbextensions_configurator in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (0.4.1)\n",
      "Requirement already satisfied: tornado in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jupyter_nbextensions_configurator) (6.0.4)\n",
      "Requirement already satisfied: jupyter-contrib-core>=0.3.3 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jupyter_nbextensions_configurator) (0.3.3)\n",
      "Requirement already satisfied: pyyaml in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jupyter_nbextensions_configurator) (5.3.1)\n",
      "Requirement already satisfied: notebook>=4.0 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jupyter_nbextensions_configurator) (6.1.1)\n",
      "Requirement already satisfied: traitlets in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jupyter_nbextensions_configurator) (4.3.3)\n",
      "Requirement already satisfied: jupyter-core in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jupyter_nbextensions_configurator) (4.6.3)\n",
      "Requirement already satisfied: setuptools in /Users/bekkkkkahhh/.local/lib/python3.7/site-packages (from jupyter-contrib-core>=0.3.3->jupyter_nbextensions_configurator) (46.4.0)\n",
      "Requirement already satisfied: nbformat in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (5.0.7)\n",
      "Requirement already satisfied: ipython-genutils in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (6.1.6)\n",
      "Requirement already satisfied: nbconvert in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (6.0.6)\n",
      "Requirement already satisfied: argon2-cffi in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (20.1.0)\n",
      "Requirement already satisfied: Send2Trash in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (0.8.0)\n",
      "Requirement already satisfied: jinja2 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (2.11.2)\n",
      "Requirement already satisfied: ipykernel in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (5.3.4)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (19.0.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (0.8.3)\n",
      "Requirement already satisfied: decorator in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from traitlets->jupyter_nbextensions_configurator) (4.4.2)\n",
      "Requirement already satisfied: six in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from traitlets->jupyter_nbextensions_configurator) (1.15.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbformat->notebook>=4.0->jupyter_nbextensions_configurator) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jupyter-client>=5.3.4->notebook>=4.0->jupyter_nbextensions_configurator) (2.8.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.3)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (2.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.6.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.5.0)\n",
      "Requirement already satisfied: bleach in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (3.2.1)\n",
      "Requirement already satisfied: testpath in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.4.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.0->jupyter_nbextensions_configurator) (1.14.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jinja2->notebook>=4.0->jupyter_nbextensions_configurator) (1.1.1)\n",
      "Requirement already satisfied: appnope; platform_system == \"Darwin\" in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.1.0)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (7.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=4.0->jupyter_nbextensions_configurator) (20.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=4.0->jupyter_nbextensions_configurator) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=4.0->jupyter_nbextensions_configurator) (1.7.0)\n",
      "Requirement already satisfied: nest-asyncio in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (1.4.0)\n",
      "Requirement already satisfied: async-generator in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (1.10)\n",
      "Requirement already satisfied: packaging in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (20.4)\n",
      "Requirement already satisfied: webencodings in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.0->jupyter_nbextensions_configurator) (2.20)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.17.2)\n",
      "Requirement already satisfied: pickleshare in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (3.0.7)\n",
      "Requirement already satisfied: backcall in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat->notebook>=4.0->jupyter_nbextensions_configurator) (3.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from packaging->bleach->nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (2.4.7)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.7.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /Users/bekkkkkahhh/opt/anaconda3/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.2.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/bekkkkkahhh/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Enabling: jupyter_nbextensions_configurator\n",
      "- Writing config: /Users/bekkkkkahhh/.jupyter\n",
      "    - Validating...\n",
      "      jupyter_nbextensions_configurator 0.4.1 \u001b[32mOK\u001b[0m\n",
      "Enabling notebook nbextension nbextensions_configurator/config_menu/main...\n",
      "Enabling tree nbextension nbextensions_configurator/tree_tab/main...\n"
     ]
    }
   ],
   "source": [
    "! pip install jupyter_nbextensions_configurator\n",
    "! jupyter nbextensions_configurator enable --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "AAi9YZfqTkF0",
    "outputId": "1245f0c5-1804-45bd-f65e-45eded65a282",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "TwPVevBrrVeH",
    "outputId": "f80162f5-2eee-461e-a757-d436835f2f1c"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/My\\ Drive/GaTech/Fall20/NLP-TA/HW5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR4YChEjJY4k"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YeZ3QArHTdsA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hvfwJpggx3G"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "Read in the pickle files provided to you. We have preprocessed and tokenized all the question answering data form you. Just run the following cells to initialize your word and char to index maps as well as training data. \n",
    "\n",
    "For `word2idx` and `char2idx`, index 0 is reserved for *<unk>* and 1 is reserved for *<pad>*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autopep8\n",
      "  Downloading autopep8-1.5.4.tar.gz (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 993 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycodestyle>=2.6.0\n",
      "  Downloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 524 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: autopep8\n",
      "  Building wheel for autopep8 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autopep8: filename=autopep8-1.5.4-py2.py3-none-any.whl size=45287 sha256=2e5b809a641ca7578ad88ae9d23117849846bef627b2961341f882bf292ae611\n",
      "  Stored in directory: /Users/bekkkkkahhh/Library/Caches/pip/wheels/2c/ad/e2/f5322a230aedd0091b75ec899404e3562d8bb4e7ba0f025cbd\n",
      "Successfully built autopep8\n",
      "Installing collected packages: pycodestyle, toml, autopep8\n",
      "Successfully installed autopep8-1.5.4 pycodestyle-2.6.0 toml-0.10.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/bekkkkkahhh/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install autopep8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "54_3_ChZCEJo"
   },
   "outputs": [],
   "source": [
    "word2idx_file ='data/word2idx.pickle'\n",
    "char2idx_file = 'data/char2idx.pickle'\n",
    "train_pkl = 'data/train-v2.0.json'\n",
    "valid_pkl = 'data/dev-v2.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "w-CyOklbxXN7",
    "outputId": "75103ca7-ab00-4ef6-a7a2-afee8a0f2449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 23284\n",
      "<unk> 0\n",
      "<pad> 1\n"
     ]
    }
   ],
   "source": [
    "word2idx = pickle.load(open(word2idx_file, 'rb')) # word2idx \n",
    "idx2word = {v : k for k, v in word2idx.items()}\n",
    "word_vocab = list(word2idx.keys())\n",
    "print(\"Vocab size:\", len(word_vocab))\n",
    "# for i in range(5):\n",
    "    # print(word_vocab[i], word2idx[word_vocab[i]], idx2word[word2idx[word_vocab[i]]])\n",
    "# Print <unk> and <pad>\n",
    "print(idx2word[0], word2idx['<unk>'])\n",
    "print(idx2word[1], word2idx['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "GnosNsVZJeTF",
    "outputId": "ea905bab-3aba-40e4-b953-374bc1489a7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_WORD_LEN = max([len(x) for x in word2idx.keys()])\n",
    "MAX_WORD_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "2pqSkCzlz6rl",
    "outputId": "c2dd7d48-3262-454e-dadc-46f3205adc82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chars: 96\n",
      "<unk> 0\n",
      "<pad> 1\n"
     ]
    }
   ],
   "source": [
    "char2idx = pickle.load(open(char2idx_file, 'rb')) # char2idx \n",
    "idx2char = {v : k for k, v in char2idx.items()}\n",
    "char_vocab = list(char2idx.keys())\n",
    "print(\"Unique chars:\", len(char_vocab))\n",
    "print(idx2char[0], char2idx['<unk>'])\n",
    "print(idx2char[1], char2idx['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4b4a964b8fb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSQuAD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSQUADwithGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "import util\n",
    "from util import collate_fn, collate_fn_graph,SQuAD,SQUADwithGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "QGQyJfQrYMJY",
    "outputId": "b8dd259d-3458-4b38-85b6-d86e400ca681"
   },
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '{'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ed294da8c72a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train samples\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Valid samples\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '{'."
     ]
    }
   ],
   "source": [
    "val_df = pd.read_pickle(valid_pkl).reset_index(drop=True)\n",
    "train_df = pd.read_pickle(train_pkl).reset_index(drop=True)\n",
    "print(\"Train samples\", len(train_df))\n",
    "print(\"Valid samples\", len(val_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "LFx4g9oxZDJb",
    "outputId": "72c090a1-ecd8-4ba5-befb-f642e4561cd9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bef7ad6b9bcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Question:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Question Ids:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer Ids:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "sample = train_df.sample(1).iloc[0]\n",
    "print(\"Question:\", sample.question)\n",
    "print(\"Question Ids:\", sample.question_ids)\n",
    "print(\"Answer:\", sample.answer)\n",
    "print(\"Answer Ids:\", [word2idx[x] for x in sample.answer.split()])\n",
    "[char_start, char_end] = sample.label\n",
    "print(\"Context:\", sample.context[char_start - 30 : char_end + 30])\n",
    "[token_start, token_end] = sample.label_idx\n",
    "print(\"Context Ids:\", sample.context_ids[token_start - 5: token_end + 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "EBP9XNT8bkNW",
    "outputId": "6a956646-0bdb-4fb4-c777-5d3a23e5865c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples 8504\n",
      "Val samples 2013\n"
     ]
    }
   ],
   "source": [
    "# Counts\n",
    "print(\"Train samples\", len(train_df))\n",
    "print(\"Val samples\", len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "56ddde6b9a695914005b9628                     [France, France, France, France]\n",
       "56ddde6b9a695914005b9629    [10th and 11th centuries, in the 10th and 11th...\n",
       "56ddde6b9a695914005b962a    [Denmark, Iceland and Norway, Denmark, Iceland...\n",
       "56ddde6b9a695914005b962b                         [Rollo, Rollo, Rollo, Rollo]\n",
       "56ddde6b9a695914005b962c    [10th century, the first half of the 10th cent...\n",
       "Name: answer, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_val_answers = val_df[['id', 'answer']].groupby('id')['answer'].apply(list)\n",
    "gold_val_answers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmeReyXq0aiS"
   },
   "source": [
    "# 1. BiLSTM + Attention (same as [BiDAF](https://arxiv.org/abs/1611.01603) but different encoder)\n",
    "\n",
    "> Let's implement a variant of BiDAF using the same LSTM code from HW4 as encoder. \n",
    "\n",
    "\n",
    "<img src=\"img/BiDAF.png\">\n",
    "\n",
    "Here, we will swap out *Contextual Embed Layer*, *Word Embed Layer*, and *Character Embed Layer* with our own LSTM-based encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qBA4-sLXgX0B"
   },
   "outputs": [],
   "source": [
    "# you will need to import again after every change you make\n",
    "#    autoreload does not work here for some reason (let us \n",
    "#    know on piazza if you figure out a fix)\n",
    "from MyQA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleInput = train_df.iloc[0]\n",
    "context = sampleInput.context_ids # tokenized context converted to ids\n",
    "question = sampleInput.question_ids # tokenized query converted to ids\n",
    "labels = sampleInput.label_idx # [start_idx, end_idx] of answer\n",
    "# create char inputs for context and question\n",
    "context_chars = []\n",
    "for w in context:\n",
    "    w = idx2word[w]\n",
    "    padding = ['<pad>'] * (MAX_WORD_LEN - len(w))\n",
    "    context_chars += padding + list(w)\n",
    "context_chars = prepare_sequence(context_chars, char2idx)\n",
    "\n",
    "question_chars = []\n",
    "for w in question:\n",
    "    w = idx2word[w]\n",
    "    padding = ['<pad>'] * (MAX_WORD_LEN - len(w))\n",
    "    question_chars += padding + list(w)\n",
    "question_chars = prepare_sequence(question_chars, char2idx)\n",
    "context = torch.tensor(context, dtype=torch.long)\n",
    "question = torch.tensor(question, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xdUazL3fbODI"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters (you will need to change these)\n",
    "# Encoding layer\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 3\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 2\n",
    "CHAR_EMBEDDING_DIM = 3\n",
    "CHAR_HIDDEN_DIM = 3\n",
    "BIDIRECTIONAL_LSTM = True\n",
    "# Modeling layer\n",
    "LSTM_LAYERS_MODELING = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Encoder\n",
    "\n",
    "We will use the same POSTagger architecture to encode our passage and questions. The minor distinction here is, you have to encode inputs twice. Once for passage (\"context\") and one for the question (\"query\"). *Check that this works before moving on the the next part.*\n",
    "We will also omit the HighWay Layer for this HW. However, you are welcome to add it for Part 2. \n",
    "\n",
    "Implement code in `LSTMEncoder` class in `MyQA.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "directions = 1 if not BIDIRECTIONAL_LSTM else 2\n",
    "encode = LSTMEncoder(EMBEDDING_DIM, HIDDEN_DIM, \n",
    "                     CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, \n",
    "                     len(char2idx), len(word2idx), lstm_layers=LSTM_LAYERS, \n",
    "                     bidirectional=BIDIRECTIONAL_LSTM, dropout=DROPOUT)\n",
    "context_enc, query_enc = encode(context, context_chars, question, question_chars)\n",
    "\n",
    "# verify this with various hyperparameters (by running this cell multiple times)\n",
    "\n",
    "assert context_enc.shape == (len(context), HIDDEN_DIM*directions)\n",
    "assert query_enc.shape == (len(question), HIDDEN_DIM*directions)\n",
    "\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Attention Layer\n",
    "\n",
    "Implement the Attention Flow Layer same as in BiDAF paper inside `AttentionFlow`. More instructions are included in `MyQA.py`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([149, 24])\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "attention = AttentionFlow(HIDDEN_DIM * directions * 3)\n",
    "G = attention(context_enc, query_enc)\n",
    "print(G.shape)\n",
    "\n",
    "# verify this with various hyperparameters (by running this cell multiple times)\n",
    "assert G.shape == (len(context), HIDDEN_DIM * directions * 4)\n",
    "print(\"SUCCESS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Modeling Layer\n",
    "\n",
    "Implement the `ModelingLayer` class in `MyQA.py` same as in BiDAF paper. More instructions are included in `MyQA.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "modeling = ModelingLayer(HIDDEN_DIM * directions * 4, HIDDEN_DIM, \n",
    "                         num_layers=LSTM_LAYERS_MODELING, dropout=DROPOUT, \n",
    "                         bidirectional=BIDIRECTIONAL_LSTM)\n",
    "M = modeling(G)\n",
    "# verify this with various hyperparameters (by running this cell multiple times)\n",
    "assert M.shape == (len(context), HIDDEN_DIM * directions)\n",
    "\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Output Layer\n",
    "\n",
    "Implement the `OutputLayer` class in `MyQA.py` that will yield start_idx and end_idx vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "output = OutputLayer(HIDDEN_DIM*10, HIDDEN_DIM * directions, \n",
    "                     HIDDEN_DIM, bidirectional=BIDIRECTIONAL_LSTM)\n",
    "\n",
    "start, end = output(G, M)\n",
    "\n",
    "# verify this with various hyperparameters (by running this cell multiple times)\n",
    "start = start.unsqueeze(0)\n",
    "end = end.unsqueeze(0)\n",
    "\n",
    "assert start.shape == (1, len(context))\n",
    "assert end.shape == (1, len(context))\n",
    "\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally implement the `BiDAF` class\n",
    "\n",
    "Now combine all the modules you just wrote into `BiDAF` class in `MyQA.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "model = BiDAF(EMBEDDING_DIM, HIDDEN_DIM, CHAR_EMBEDDING_DIM, \n",
    "               CHAR_HIDDEN_DIM, len(char2idx), len(word2idx), \n",
    "               bidirectional=BIDIRECTIONAL_LSTM, phrase_LSTM_layers=LSTM_LAYERS, \n",
    "               modeling_LSTM_layers=LSTM_LAYERS_MODELING, dropout=DROPOUT)\n",
    "\n",
    "start, end = model(context, context_chars, question, question_chars)\n",
    "start = start.unsqueeze(0)\n",
    "end = end.unsqueeze(0)\n",
    "\n",
    "# verify this with various hyperparameters (by running this cell multiple times)\n",
    "assert start.shape == (1, len(context))\n",
    "assert end.shape == (1, len(context))\n",
    "\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement Training and Eval\n",
    "\n",
    "For this homework, you are required to just implement the train and eval functions. You don't need to train it until convergence or until it reaches high accuracy. However, as you might've noticed, the hyperparameters are the same as HW4. After implementing train and eval, your goal is to train the network for 2 epochs and tune hyperparameters so that the loss is decreasing consistently. We will verify this from your outputs. \n",
    "\n",
    "The idea here is to see the network we constructed above work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters (you will need to change these)\n",
    "# #Iter: 8503/8504\tAvg Train Loss: 9.4279\tAvg Val Loss: 9.2280\n",
    "# # Encoding layer\n",
    "# EMBEDDING_DIM = 12\n",
    "# HIDDEN_DIM = 6\n",
    "# LEARNING_RATE = 0.005\n",
    "# LSTM_LAYERS = 1\n",
    "# DROPOUT = 0.1\n",
    "# EPOCHS = 2\n",
    "# CHAR_EMBEDDING_DIM = 6\n",
    "# CHAR_HIDDEN_DIM = 6\n",
    "# BIDIRECTIONAL_LSTM = True\n",
    "# # Modeling layer\n",
    "# LSTM_LAYERS_MODELING = 2\n",
    "\n",
    "# Hyperparameters (you will need to change these)\n",
    "# Iter: 8503/8504\tAvg Train Loss: 9.2655\tAvg Val Loss: 9.0793\n",
    "# Encoding layer\n",
    "# EMBEDDING_DIM = 32\n",
    "# HIDDEN_DIM = 16\n",
    "# LEARNING_RATE = 0.01\n",
    "# LSTM_LAYERS = 2\n",
    "# DROPOUT = 0.1\n",
    "# EPOCHS = 2\n",
    "# CHAR_EMBEDDING_DIM = 16\n",
    "# CHAR_HIDDEN_DIM = 16\n",
    "# BIDIRECTIONAL_LSTM = True\n",
    "# # Modeling layer\n",
    "# LSTM_LAYERS_MODELING = 2\n",
    "\n",
    "#Epoch1: Iter: 8503/8504\tAvg Train Loss: 9.3010\tAvg Val Loss: 9.0261\n",
    "#Epoch2: Iter: 8503/8504\tAvg Train Loss: 8.8123\tAvg Val Loss: 8.7616\n",
    "# Encoding layer\n",
    "# EMBEDDING_DIM = 64\n",
    "# HIDDEN_DIM = 32\n",
    "# LEARNING_RATE = 0.01\n",
    "# LSTM_LAYERS = 2\n",
    "# DROPOUT = 0.1\n",
    "# EPOCHS = 2\n",
    "# CHAR_EMBEDDING_DIM = 32\n",
    "# CHAR_HIDDEN_DIM = 32\n",
    "# BIDIRECTIONAL_LSTM = True\n",
    "# # Modeling layer\n",
    "# LSTM_LAYERS_MODELING = 2\n",
    "\n",
    "\n",
    "# Encoding layer\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 0.01\n",
    "LSTM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 2\n",
    "CHAR_EMBEDDING_DIM = 64\n",
    "CHAR_HIDDEN_DIM = 64\n",
    "BIDIRECTIONAL_LSTM = True\n",
    "# Modeling layer\n",
    "LSTM_LAYERS_MODELING = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_scores(predictions):\n",
    "    \"\"\"2D list of [[ID, PRED], [ID, PRED], ...]\"\"\"\n",
    "    f1 = exact_match = 0\n",
    "    for row in predictions:\n",
    "        [ID, PRED] = row\n",
    "        goldans = gold_val_answers.loc[ID]\n",
    "        f1 += max(compute_f1(g, PRED) for g in goldans)\n",
    "        exact_match += max(compute_exact(g, PRED) for g in goldans)\n",
    "    num_samples = len(predictions)\n",
    "    return f1 / num_samples, exact_match / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "code_folding": [],
    "id": "VKHYPwgWBB5y",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256513d581ff447fa88ed93e9eb4907f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8504.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context torch.Size([149])\n",
      "context torch.Size([149])\n",
      "context torch.Size([149])\n",
      "context torch.Size([149])\n",
      "context torch.Size([149])\n",
      "context torch.Size([149])\n",
      "context torch.Size([149])\n",
      "context torch.Size([149])\n",
      "context torch.Size([149])\n",
      "context torch.Size([149])\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-b5c3b459f4be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;31m# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_BiDAF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;31m#     eval_BiDAF(model, loss_function, optimizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-b5c3b459f4be>\u001b[0m in \u001b[0;36mtrain_BiDAF\u001b[0;34m(epoch, model, loss_function, optimizer)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/HW5/MyQA.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, context, context_chars, query, query_chars)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;31m# encode the words and chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mcontext_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# use attention layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/HW5/MyQA.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, context, context_chars, query, query_chars)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# lstm for char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0moutputs_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# cat word embed with lstm[-1] of chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 727\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_BiDAF(epoch, model, loss_function, optimizer):\n",
    "    train_loss = 0\n",
    "    train_examples = len(train_df)\n",
    "    itr = 0\n",
    "    for i, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "        # for i, row in train_df.iterrows():\n",
    "        context = row.context_ids # tokenized context converted to ids\n",
    "        question = row.question_ids # tokenized query converted to ids\n",
    "        labels = row.label_idx # [start_idx, end_idx] of answer\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Find the gradient with respect to the loss and update the model \n",
    "        #   parameters using the optimizer.\n",
    "        # `context` and `question` are both already tokenized and converted to ids\n",
    "        # You need to prepare your character level input. Feel free to use the \n",
    "        #   prepare_sequence method for this. \n",
    "        #############################################################################\n",
    "        \n",
    "        # create char inputs for context and question           \n",
    "        context_chars = []\n",
    "        for w in context:\n",
    "            w = idx2word[w]\n",
    "            padding = ['<pad>'] * (MAX_WORD_LEN - len(w))\n",
    "            context_chars += padding + list(w)\n",
    "        context_chars = prepare_sequence(context_chars, char2idx)\n",
    "\n",
    "        question_chars = []\n",
    "        for w in question:\n",
    "            w = idx2word[w]\n",
    "            padding = ['<pad>'] * (MAX_WORD_LEN - len(w))\n",
    "            question_chars += padding + list(w)\n",
    "        question_chars = prepare_sequence(question_chars, char2idx)\n",
    "        \n",
    "        # use the model\n",
    "        context = torch.tensor(context, dtype=torch.long)\n",
    "        print('context', context.shape)\n",
    "        question = torch.tensor(question, dtype=torch.long)\n",
    "        start, end = model(context, context_chars, question, question_chars)\n",
    "        \n",
    "        # prepare data\n",
    "        start = start.reshape(1,-1)\n",
    "        end = end.reshape(1,-1)\n",
    "        labels = torch.tensor(labels)\n",
    "        se = torch.cat((start, end), dim=0)\n",
    "        \n",
    "        # caculate the loss here\n",
    "#         loss = 2 * loss_function(se, labels)\n",
    "        loss = loss_function(se, labels)\n",
    "        \n",
    "        # update the loss\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update train_loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "#         print(torch.norm(model.outputLayer.fc_GM.weight))\n",
    "               \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        if ((i+1) % 100) == 0:\n",
    "            avg_train_loss = train_loss / i\n",
    "#             print(\"\\nLoss:\", avg_train_loss)\n",
    "            avg_val_loss, f1_avg, exact_match_avg = eval_BiDAF(model, loss_function, rand_samples=10)\n",
    "            print(\"Iter: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\tVal F1: {:.0f} \\tVal ExactMatch: {:.0f}\".format(i, train_examples, avg_train_loss, avg_val_loss, f1_avg, exact_match_avg))\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, f1_avg, exact_match_avg = eval_BiDAF(model, loss_function, rand_samples=1000)        \n",
    "    print(\"Iter: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\tVal F1: {:.0f} \\tVal ExactMatch: {:.0f}\".format(i, train_examples, avg_train_loss, avg_val_loss, f1_avg, exact_match_avg))\n",
    "    return model\n",
    "\n",
    "def eval_BiDAF(model, loss_function, rand_samples=-1):\n",
    "    val_loss = 0\n",
    "    if rand_samples != -1:\n",
    "        validation_set = val_df.sample(rand_samples)\n",
    "    else:\n",
    "        validation_set = val_df\n",
    "    val_examples = len(validation_set)\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i, row in tqdm(validation_set.iterrows(), total=len(validation_set)):\n",
    "            # for i, row in validation_set.iterrows(): # if tqdm is not installed \n",
    "            context = row.context_ids # tokenized context converted to ids\n",
    "            orig_context = prepare_sequence(context, idx2word)\n",
    "            question = row.question_ids # tokenized query converted to ids\n",
    "            [true_start, true_end] = row.label_idx # [start_idx, end_idx] of answer\n",
    "            pred_start, pred_end = None, None # maxidx of the softmax outputs\n",
    "            #############################################################################\n",
    "            # TODO: Implement the training loop\n",
    "            # Find the gradient with respect to the loss and update the model \n",
    "            #   parameters using the optimizer.\n",
    "            # `context` and `question` are both already tokenized and converted to ids\n",
    "            # You need to prepare your character level input. Feel free to use the \n",
    "            #   prepare_sequence method for this. \n",
    "            #############################################################################\n",
    "            \n",
    "            # create char inputs for context and question           \n",
    "            context_chars = []\n",
    "            for w in context:\n",
    "                w = idx2word[w]\n",
    "                padding = ['<pad>'] * (MAX_WORD_LEN - len(w))\n",
    "                context_chars += padding + list(w)\n",
    "            context_chars = prepare_sequence(context_chars, char2idx)\n",
    "\n",
    "            question_chars = []\n",
    "            for w in question:\n",
    "                w = idx2word[w]\n",
    "                padding = ['<pad>'] * (MAX_WORD_LEN - len(w))\n",
    "                question_chars += padding + list(w)\n",
    "            question_chars = prepare_sequence(question_chars, char2idx)\n",
    "\n",
    "            # use the model\n",
    "            context = torch.tensor(context, dtype=torch.long)\n",
    "            question = torch.tensor(question, dtype=torch.long)\n",
    "            start, end = model(context, context_chars, question, question_chars)\n",
    "            \n",
    "            # prepare data\n",
    "            start = start.reshape(1,-1)\n",
    "            end = end.reshape(1,-1)\n",
    "            labels = torch.tensor([true_start, true_end])\n",
    "            se = torch.cat((start, end), dim=0)\n",
    "\n",
    "            # caculate the loss here\n",
    "            loss = loss_function(se, labels)\n",
    "            \n",
    "            #update loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # get the prediction here.\n",
    "            pred_start, pred_end = int(start.argmax(dim = 1)[0]), int(end.argmax(dim = 1)[0])\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            predictions.append([row.id, orig_context[pred_start : pred_end + 1]])\n",
    "            \n",
    "    f1_avg, exact_match_avg = get_eval_scores(predictions)\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, f1_avg*100, exact_match_avg*100\n",
    "\n",
    "#############################################################################\n",
    "# TODO: define loss function and optimizer to be used in train_BiDAF \n",
    "#############################################################################\n",
    "model = BiDAF(EMBEDDING_DIM, HIDDEN_DIM, CHAR_EMBEDDING_DIM, \n",
    "               CHAR_HIDDEN_DIM, len(char2idx), len(word2idx), \n",
    "               bidirectional=BIDIRECTIONAL_LSTM, phrase_LSTM_layers=LSTM_LAYERS, \n",
    "               modeling_LSTM_layers=LSTM_LAYERS_MODELING, dropout=DROPOUT)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def loss_fn(data, labels):\n",
    "    loss = Variable(torch.zeros(1))\n",
    "    for d, label in zip(data, labels):\n",
    "        loss -= torch.log(d[label]).cpu()\n",
    "    return loss\n",
    "\n",
    "# Define loss function\n",
    "loss_function = loss_fn\n",
    "\n",
    "# Define optimizer\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        \n",
    "    # in the paper: We use the AdaDelta (Zeiler, 2012) optimizer\n",
    "    # with a minibatch size of 60 and an initial learning rate of 0.5, for 12 epochs.\n",
    "\n",
    "optimizer = optim.Adadelta(parameters, lr=LEARNING_RATE)\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model = train_BiDAF(epoch, model, loss_function, optimizer)\n",
    "#     eval_BiDAF(model, loss_function, optimizer)\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 23284)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(char2idx), len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW5_NEW.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "424px",
    "left": "1088px",
    "right": "20px",
    "top": "103px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "57d7d0e7cd1c4f3a8ed0cf291091324d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d803615bb424bb2944372affe458f6f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b39cb786f6474e598af44a42921665a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c80ba3ff9f1541ac876dc5506df07ed4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea6de71ec974455cb589154758fb964d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c80ba3ff9f1541ac876dc5506df07ed4",
      "placeholder": "​",
      "style": "IPY_MODEL_57d7d0e7cd1c4f3a8ed0cf291091324d",
      "value": " 232k/232k [00:00&lt;00:00, 308kB/s]"
     }
    },
    "efee88d526cf4249a96d8c14502e8259": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f6c66eb6e1e0425ab33d75117d65e5bd",
       "IPY_MODEL_ea6de71ec974455cb589154758fb964d"
      ],
      "layout": "IPY_MODEL_7d803615bb424bb2944372affe458f6f"
     }
    },
    "f6c4ecf71a5d46f983730ea71872346b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f6c66eb6e1e0425ab33d75117d65e5bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b39cb786f6474e598af44a42921665a3",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6c4ecf71a5d46f983730ea71872346b",
      "value": 231508
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
