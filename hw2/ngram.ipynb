{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Assigment 2\n",
    "# N-gram Language Models\n",
    "\n",
    "In the textbook, language modeling was defined as the task of predicting the next word in a sequence given the previous words. In this assignment, we will focus on the related problem of predicting the next *character* or *word* in a sequence given the previous characters. \n",
    "\n",
    "The learning goals of this assignment are to: \n",
    "* Understand how to compute language model probabilities using maximum likelihood estimation.\n",
    "* Implement basic smoothing, back-off and interpolation.\n",
    "* Have fun using a language model to probabilistically generate texts.\n",
    "* Compare word-level langauage models and character-level language models\n",
    "\n",
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "Here are the materials that you should download for this assignment:\n",
    "\n",
    "* [Skeleton python code](https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/programming/HW2/skeleton.zip).\n",
    "\n",
    "* [training data for character-level and word-level langauge models](https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/programming/HW2/shakespeare_input.zip).\n",
    "\n",
    "* [dev data for character-level and word-level langauge models](https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/programming/HW2/shakespeare_sonnets.zip).\n",
    "\n",
    "* [training data for word-level langauge models](https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/programming/HW2/train_e.zip).\n",
    "\n",
    "* [dev data for word-level langauge models](https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/programming/HW2/val_e.zip).\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level N-gram Language Models [20 pts]\n",
    "\n",
    "You should complete functions in the script `hw2_skeleton_char.py` in this part. After submitting `hw2_skeleton_char.py` to Gradescope and passing all the test cases for this part, you can get full score.\n",
    "\n",
    "## Part 1: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from skeleton.hw2_skeleton_char import ngrams,NgramModel,create_ngram_model,NgramModelWithInterpolation\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `ngrams(n, text)` that produces a list of all n-grams of the specified size from the input text. Each n-gram should consist of a 2-element tuple `(context, char)`, where the context is itself an n-length string comprised of the $n$ characters preceding the current character. The sentence should be padded with $n$ ~ characters at the beginning (we've provided you with `start_pad(n)` for this purpose). If $n=0$, all contexts should be empty strings. You may assume that $n\\ge0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('~~', 'a'), ('~a', 'b'), ('ab', 'c')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(2, 'abc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also given you the function `create_ngram_model(model_class, path, n, k)` that will create and return an n-gram model trained on the entire file path provided. You should use it.\n",
    "\n",
    "You will build a simple n-gram language model that can be used to generate random text resembling a source document. Your use of external code should be limited to built-in Python modules, which excludes, for example, NumPy and NLTK.\n",
    "\n",
    "1. In the `NgramModel` class, write an initialization method `__init__(self, n, k)` which stores the order $n$ of the model and initializes any necessary internal variables. Then write a method `get_vocab(self)` that returns the vocab (this is the set of all characters used by this model).\n",
    "\n",
    "2. Write a method `update(self, text)` which computes the n-grams for the input sentence and updates the internal counts. Also write a method `prob(self, context, char)` which accepts an n-length string representing a context and a character, and returns the probability of that character occuring, given the preceding context. If you encounter a novel `context`, the probability of any given `char` should be $1/V$ where $V$ is the size of the vocab.\n",
    "\n",
    "3. Write a method `random_char(self, context)` which returns a random character according to the probability distribution determined by the given context. Specifically, let $V=\\langle v_1,v_2, \\cdots, v_n \\rangle$ be the vocab, sorted according to Python's natural lexicographic ordering, and let $0\\le r<1$ be a random number between 0 and 1. Your method should return the character $v_i$ such that\n",
    "\n",
    "    $$\\sum_{j=1}^{i-1} P(v_j\\ |\\ \\text{context}) \\le r < \\sum_{j=1}^i P(v_j\\ | \\ \\text{context}).$$\n",
    "\n",
    "    You should use a single call to the `random.random()` function to generate $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.pow(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = NgramModel(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.update('abab')\n",
    "m.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c', 'd'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.update('abcd')\n",
    "m.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.prob('a', 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.prob('~', 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.prob('b', 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.update('abab')\n",
    "m.update('abcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c',\n",
       " 'c']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random.seed(1)\n",
    "sorted([m.random_char('b') for i in range(25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random.seed(1)\n",
    "m.random_char('abcd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In the `NgramModel` class, write a method `random_text(self, length)` which returns a string of characters chosen at random using the `random_char(self, context)` method. Your starting context should always be $n$ ~ characters, and the context should be updated as characters are generated. If $n=0$, your context should always be the empty string. You should continue generating characters until you've produced the specified number of random characters, then return the full string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdbabcbabababcdddaabcda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skeleton.hw2_skeleton_char import ngrams,NgramModel,create_ngram_model,NgramModelWithInterpolation\n",
    "import random\n",
    "m = NgramModel(1, 0.1)\n",
    "m.update('abab')\n",
    "m.update('abcd')\n",
    "random.seed(1)\n",
    "m.random_text(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Shakespeare \n",
    "\n",
    "Now you can train a language model. First grab some text like [this corpus of Shakespeare](https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/programming/HW2/shakespeare_input.zip).\n",
    "\n",
    "Try generating some Shakespeare with different order n-gram models. You should try running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fir of yould brin most some a wou to gety.\\n\\nShavere laguis prinsaved,\\nAEGARMACURESS Andeavelf it teronot wromper'd swe sur\\nOphy, sh tholovence er hisoleasou aing forrat struchis shost of you are rock a be mothatilly shus,\\n'Twom,\\nCHARUTUS:\\nAbou, now'd\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 2)\n",
    "m.random_text(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First\\nthe deliege\\n'Twas empers.\\n\\nAlarent\\nCall no heave gents a gentlemean he a rus man.\\n\\nIAGO:\\nUnless, woul finding why he powere to to my form brothat face,\\nThen, there's a good\\nrevert oft in\\nand dumpetire.\\n\\nTAMORENCE:\\nYou speak wildeny you\\nsee daug\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 3)\n",
    "m.random_text(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Service;\\nFor me awhilst.\\n\\nSEBASTINGS:\\nHer be the true, when\\nAmbitious and followers, none.\\nA sing stubborn imprisoner. Ford; Mistress, where:\\nShe action! Belaribel touch is but thence\\nAnd wants: have thou say, ever did, Lord Bardon myself the c'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 4)\n",
    "m.random_text(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the reason why it is started with 'F'\n",
    "m.ngrams_char_count\n",
    "m.prob('~~~~','F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Is it as good as [1000 monkeys working at 1000 typewriters](https://www.youtube.com/watch?v=no_elVGGgW8)?\n",
    "\n",
    "After generating a bunch of short passages, do you notice anything? *They all start with F!* In fact, after we hit a certain order, the first word is always *First*?  Why is that? Is the model trying to be clever? First, generate the word *First*. Explain what is going on in your writeup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Perplexity, Smoothing, and Interpolation\n",
    "\n",
    "In this part of the assignment, you'll adapt your code in order to implement several of the  techniques described in [Section 3 of the Jurafsky and Martin textbook](https://web.stanford.edu/~jurafsky/slp3/3.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "How do we know whether a language model is good? There are two basic approaches:\n",
    "1. Task-based evaluation (also known as extrinsic evaluation), where we use the language model as part of some other task, like automatic speech recognition, or spelling correcktion, or an OCR system that tries to covert a professor's messy handwriting into text.\n",
    "2. Intrinsic evaluation. Intrinsic evaluation tries to directly evalute the goodness of the language model by seeing how well the probability distributions that it estimates are able to explain some previously unseen test set.\n",
    "\n",
    "Here's what the textbook says:\n",
    "\n",
    "> For an intrinsic evaluation of a language model we need a test set. As with many of the statistical models in our field, the probabilities of an N-gram model come from the corpus it is trained on, the training set or training corpus. We can then measure the quality of an N-gram model by its performance on some unseen data called the test set or test corpus. We will also sometimes call test sets and other datasets that are not in our training sets held out corpora because we hold them out from the training data.\n",
    "\n",
    "> So if we are given a corpus of text and want to compare two different N-gram models, we divide the data into training and test sets, train the parameters of both models on the training set, and then compare how well the two trained models fit the test set.\n",
    "\n",
    "> But what does it mean to \"fit the test set\"? The answer is simple: whichever model assigns a higher probability to the test set is a better model.\n",
    "\n",
    "We'll implement the most common method for intrinsic metric of language models: *perplexity*.  The perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of characters. For a test set $$W = w_1 w_2 ... w_N$$:\n",
    "\n",
    "$$Perplexity(W) = P(w_1 w_2 ... w_N)^{-\\frac{1}{N}}$$\n",
    "\n",
    "$$ = \\sqrt[N]{\\frac{1}{P(w_1 w_2 ... w_N)}}$$\n",
    "\n",
    "$$ = \\sqrt[N]{\\prod_{i=1}^{N}{\\frac{1}{P(w_i \\mid w_1 ... w_{i-1})}}}$$\n",
    "\n",
    "Now implement the `perplexity(self, text)` function in `NgramModel`. A couple of things to keep in mind:\n",
    "1. Numeric underflow is going to be a problem, so consider using logs.\n",
    "2. Perplexity is undefined if the language model assigns any zero probabilities to the test set. In that case your code should return positive infinity - `float('inf')`.\n",
    "3. On your unsmoothed models, you'll definitely get some zero probabilities for the test set. To test you code, you should try computing perplexity on the training set, and you should compute perplexity for your language models that use smoothing and interpolation.\n",
    "\n",
    "We provide you [dev data for character-level and word-level langauge models](https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/programming/HW2/shakespeare_sonnets.zip).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5874010519681994"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skeleton.hw2_skeleton_char import ngrams,NgramModel,create_ngram_model,NgramModelWithInterpolation\n",
    "import random\n",
    "m = NgramModel(1, 0)\n",
    "m.update('abcd')\n",
    "m.update('acbd')\n",
    "m.perplexity('acd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.perplexity('abca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.910100720888779\n"
     ]
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 2, k=1)\n",
    "with open('shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n",
    "    print(m.perplexity(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you may want to create a smoothed language model before calculating perplexity on real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "\n",
    "Laplace Smoothing is described in section 4.4.1. Laplace smoothing adds one to each count (hence its alternate name *add-one smoothing*). Since there are *V* characters in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations.\n",
    "\n",
    "$$P_{Laplace}(w_i) = \\frac{count_i + 1}{N+|V|}$$\n",
    "\n",
    "A variant of Laplace smoothing is called *Add-k smoothing* or *Add-epsilon smoothing*. This is described in section Add-k 4.4.2. Update your `NgramModel` code from Part 1 to implement add-k smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m = NgramModel(1, 1)\n",
    "m.update('abab')\n",
    "m.update('abcd')\n",
    "m.prob('a', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.691781635477648"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.perplexity('abca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "7.996946415762477\n"
     ]
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 2, k=0.1)\n",
    "print(len(m.get_vocab()))\n",
    "with open('shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n",
    "    print(m.perplexity(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "\n",
    "The idea of interpolation is to calculate the higher order n-gram probabilities also combining the probabilities for lower-order n-gram models. Like smoothing, this helps us avoid the problem of zeros if we haven't observed the longer sequence in our training data. Here's the math:\n",
    "\n",
    "$$P_{interpolation}(w_i|w_{i−2} w_{i−1}) = \\lambda_1 P(w_i|w_{i−2} w_{i−1}) + \\lambda_2 P(w_i|w_{i−1}) + \\lambda_3 P(w_i)$$\n",
    "\n",
    "where $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$.\n",
    "\n",
    "We've provided you with another class definition `NgramModelWithInterpolation` that extends `NgramModel` for you to implement interpolation. If you've written your code robustly, you should only need to override the `get_vocab(self)`, `update(self, text)`, and `prob(self, context, char)` methods, along with the initializer.\n",
    "\n",
    "The value of $n$ passed into `__init__(self, n, k)` is the highest order n-gram to be considered by the model (e.g. $n=2$ will consider 3 different length n-grams). Add-k smoothing should take place only when calculating the individual order n-gram probabilities, not when calculating the overall interpolation probability.\n",
    "\n",
    "By default set the lambdas to be equal weights, but you should also write a helper function that can be called to overwrite this default. Setting the lambdas in the helper function can either be done heuristically or by using a development set, but in the example code below, we've used the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 8, '~': 2, 'a': 3, 'b': 2, '~~': 2, '~a': 2, 'ab': 2, 'ba': 1, 'c': 1, 'bc': 1}\n",
      "---\n",
      "{'': {'a': 3, 'b': 3, 'c': 1, 'd': 1}, '~': {'a': 2}, 'a': {'b': 3}, 'b': {'a': 1, 'c': 1}, '~~': {'a': 2}, '~a': {'b': 2}, 'ab': {'a': 1, 'c': 1}, 'ba': {'b': 1}, 'c': {'d': 1}, 'bc': {'d': 1}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4583333333333333"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skeleton.hw2_skeleton_char import ngrams,NgramModel,create_ngram_model,NgramModelWithInterpolation\n",
    "import random\n",
    "m = NgramModelWithInterpolation(2, 0)\n",
    "\n",
    "m.update('abab')\n",
    "m.update('abcd')\n",
    "print(m.ngrams)\n",
    "print('---')\n",
    "print(m.ngrams_char_count)\n",
    "m.prob('ab', 'a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.375]\n",
      "[0.3333333333333333, 0.3333333333333333, 0.125]\n",
      "[1.0, 1.0, 0.375]\n",
      "[0.3333333333333333, 0.3333333333333333, 0.125]\n",
      "[0.5, 0.5, 0.125]\n",
      "[0.16666666666666666, 0.16666666666666666, 0.041666666666666664]\n",
      "[0.0, 0.0, 0.375]\n",
      "[0.0, 0.0, 0.125]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4154246840757705"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.set_lambda([1/3, 1/3, 1/3])\n",
    "m.perplexity('abca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5714285714285714, 0.3333333333333333]\n",
      "[0.16666666666666666, 0.19047619047619047, 0.1111111111111111]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4682539682539682"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = NgramModelWithInterpolation(2, 1)\n",
    "m.update('abab')\n",
    "m.update('abcd')\n",
    "m.prob('~a', 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9003863011784152"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.perplexity('abca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "k is  0.01\n",
      "n is  1\n",
      "lambda is  [1, 0]\n",
      "12.507064670984176\n",
      "======\n",
      "k is  0.01\n",
      "n is  1\n",
      "lambda is  [0.8, 0.2]\n",
      "12.377156380683624\n",
      "======\n",
      "k is  0.01\n",
      "n is  1\n",
      "lambda is  [0.5, 0.5]\n",
      "14.268139831989634\n",
      "======\n",
      "k is  0.01\n",
      "n is  1\n",
      "lambda is  [0.2, 0.8]\n",
      "17.928968715826137\n",
      "======\n",
      "k is  0.05\n",
      "n is  1\n",
      "lambda is  [1, 0]\n",
      "12.414256866324168\n",
      "======\n",
      "k is  0.05\n",
      "n is  1\n",
      "lambda is  [0.8, 0.2]\n",
      "12.375785519209728\n",
      "======\n",
      "k is  0.05\n",
      "n is  1\n",
      "lambda is  [0.5, 0.5]\n",
      "14.267556112464746\n",
      "======\n",
      "k is  0.05\n",
      "n is  1\n",
      "lambda is  [0.2, 0.8]\n",
      "17.928824714810624\n",
      "======\n",
      "k is  0.1\n",
      "n is  1\n",
      "lambda is  [1, 0]\n",
      "12.374521854308794\n",
      "======\n",
      "k is  0.1\n",
      "n is  1\n",
      "lambda is  [0.8, 0.2]\n",
      "12.375152761290542\n",
      "======\n",
      "k is  0.1\n",
      "n is  1\n",
      "lambda is  [0.5, 0.5]\n",
      "14.267369215374984\n",
      "======\n",
      "k is  0.1\n",
      "n is  1\n",
      "lambda is  [0.2, 0.8]\n",
      "17.92887276653698\n",
      "======\n",
      "k is  0.5\n",
      "n is  1\n",
      "lambda is  [1, 0]\n",
      "12.281936233797339\n",
      "======\n",
      "k is  0.5\n",
      "n is  1\n",
      "lambda is  [0.8, 0.2]\n",
      "12.374048269480218\n",
      "======\n",
      "k is  0.5\n",
      "n is  1\n",
      "lambda is  [0.5, 0.5]\n",
      "14.267795953956414\n",
      "======\n",
      "k is  0.5\n",
      "n is  1\n",
      "lambda is  [0.2, 0.8]\n",
      "17.92990654314159\n",
      "======\n",
      "k is  0.01\n",
      "n is  2\n",
      "lambda is  [1, 0, 0]\n",
      "8.168500693819311\n",
      "======\n",
      "k is  0.01\n",
      "n is  2\n",
      "lambda is  [0.8, 0.1, 0.1]\n",
      "8.178454737941307\n",
      "======\n",
      "k is  0.01\n",
      "n is  2\n",
      "lambda is  [0.5, 0.25, 0.25]\n",
      "9.299300769993232\n",
      "======\n",
      "k is  0.01\n",
      "n is  2\n",
      "lambda is  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "10.281360937122347\n",
      "======\n",
      "k is  0.05\n",
      "n is  2\n",
      "lambda is  [1, 0, 0]\n",
      "8.043923839514484\n",
      "======\n",
      "k is  0.05\n",
      "n is  2\n",
      "lambda is  [0.8, 0.1, 0.1]\n",
      "8.173909060838643\n",
      "======\n",
      "k is  0.05\n",
      "n is  2\n",
      "lambda is  [0.5, 0.25, 0.25]\n",
      "9.302002207598996\n",
      "======\n",
      "k is  0.05\n",
      "n is  2\n",
      "lambda is  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "10.285429102498762\n",
      "======\n",
      "k is  0.1\n",
      "n is  2\n",
      "lambda is  [1, 0, 0]\n",
      "7.996946415762477\n",
      "======\n",
      "k is  0.1\n",
      "n is  2\n",
      "lambda is  [0.8, 0.1, 0.1]\n",
      "8.172087031068537\n",
      "======\n",
      "k is  0.1\n",
      "n is  2\n",
      "lambda is  [0.5, 0.25, 0.25]\n",
      "9.30433969474145\n",
      "======\n",
      "k is  0.1\n",
      "n is  2\n",
      "lambda is  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "10.288650009339714\n",
      "======\n",
      "k is  0.5\n",
      "n is  2\n",
      "lambda is  [1, 0, 0]\n",
      "7.918440177460784\n",
      "======\n",
      "k is  0.5\n",
      "n is  2\n",
      "lambda is  [0.8, 0.1, 0.1]\n",
      "8.182059094410878\n",
      "======\n",
      "k is  0.5\n",
      "n is  2\n",
      "lambda is  [0.5, 0.25, 0.25]\n",
      "9.32171827198982\n",
      "======\n",
      "k is  0.5\n",
      "n is  2\n",
      "lambda is  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "10.307018627007555\n",
      "======\n",
      "k is  0.01\n",
      "n is  3\n",
      "lambda is  [1, 0, 0, 0]\n",
      "5.982505064372434\n",
      "======\n",
      "k is  0.01\n",
      "n is  3\n",
      "lambda is  [0.7, 0.1, 0.1, 0.1]\n",
      "6.198020707704571\n",
      "======\n",
      "k is  0.01\n",
      "n is  3\n",
      "lambda is  [0.5, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]\n",
      "6.774289147946472\n",
      "======\n",
      "k is  0.01\n",
      "n is  3\n",
      "lambda is  [0.25, 0.25, 0.25, 0.25]\n",
      "7.921017394650913\n",
      "======\n",
      "k is  0.05\n",
      "n is  3\n",
      "lambda is  [1, 0, 0, 0]\n",
      "5.897923971194606\n",
      "======\n",
      "k is  0.05\n",
      "n is  3\n",
      "lambda is  [0.7, 0.1, 0.1, 0.1]\n",
      "6.219714837778751\n",
      "======\n",
      "k is  0.05\n",
      "n is  3\n",
      "lambda is  [0.5, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]\n",
      "6.797011223021554\n",
      "======\n",
      "k is  0.05\n",
      "n is  3\n",
      "lambda is  [0.25, 0.25, 0.25, 0.25]\n",
      "7.941169959441386\n",
      "======\n",
      "k is  0.1\n",
      "n is  3\n",
      "lambda is  [1, 0, 0, 0]\n",
      "5.883794503961117\n",
      "======\n",
      "k is  0.1\n",
      "n is  3\n",
      "lambda is  [0.7, 0.1, 0.1, 0.1]\n",
      "6.243822605974658\n",
      "======\n",
      "k is  0.1\n",
      "n is  3\n",
      "lambda is  [0.5, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]\n",
      "6.820919179254226\n",
      "======\n",
      "k is  0.1\n",
      "n is  3\n",
      "lambda is  [0.25, 0.25, 0.25, 0.25]\n",
      "7.961566982637742\n",
      "======\n",
      "k is  0.5\n",
      "n is  3\n",
      "lambda is  [1, 0, 0, 0]\n",
      "5.976434065120228\n",
      "======\n",
      "k is  0.5\n",
      "n is  3\n",
      "lambda is  [0.7, 0.1, 0.1, 0.1]\n",
      "6.394075100801159\n",
      "======\n",
      "k is  0.5\n",
      "n is  3\n",
      "lambda is  [0.5, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]\n",
      "6.963240722827449\n",
      "======\n",
      "k is  0.5\n",
      "n is  3\n",
      "lambda is  [0.25, 0.25, 0.25, 0.25]\n",
      "8.078571417764504\n",
      "======\n",
      "k is  0.01\n",
      "n is  4\n",
      "lambda is  [1, 0, 0, 0, 0]\n",
      "5.529831390533658\n",
      "======\n",
      "k is  0.01\n",
      "n is  4\n",
      "lambda is  [0.6, 0.1, 0.1, 0.1, 0.1]\n",
      "5.582205209282542\n",
      "======\n",
      "k is  0.01\n",
      "n is  4\n",
      "lambda is  [0.6, 0.2, 0.1, 0.1, 0]\n",
      "5.374852400532625\n",
      "======\n",
      "k is  0.01\n",
      "n is  4\n",
      "lambda is  [0.5, 0.2, 0.2, 0.1, 0]\n",
      "5.495456257683978\n",
      "======\n",
      "k is  0.05\n",
      "n is  4\n",
      "lambda is  [1, 0, 0, 0, 0]\n",
      "5.443296431114128\n",
      "======\n",
      "k is  0.05\n",
      "n is  4\n",
      "lambda is  [0.6, 0.1, 0.1, 0.1, 0.1]\n",
      "5.659182237192908\n",
      "======\n",
      "k is  0.05\n",
      "n is  4\n",
      "lambda is  [0.6, 0.2, 0.1, 0.1, 0]\n",
      "5.405043853834618\n",
      "======\n",
      "k is  0.05\n",
      "n is  4\n",
      "lambda is  [0.5, 0.2, 0.2, 0.1, 0]\n",
      "5.521302894930793\n",
      "======\n",
      "k is  0.1\n",
      "n is  4\n",
      "lambda is  [1, 0, 0, 0, 0]\n",
      "5.484399200017653\n",
      "======\n",
      "k is  0.1\n",
      "n is  4\n",
      "lambda is  [0.6, 0.1, 0.1, 0.1, 0.1]\n",
      "5.739925210950342\n",
      "======\n",
      "k is  0.1\n",
      "n is  4\n",
      "lambda is  [0.6, 0.2, 0.1, 0.1, 0]\n",
      "5.462668854149122\n",
      "======\n",
      "k is  0.1\n",
      "n is  4\n",
      "lambda is  [0.5, 0.2, 0.2, 0.1, 0]\n",
      "5.572908549714315\n",
      "======\n",
      "k is  0.5\n",
      "n is  4\n",
      "lambda is  [1, 0, 0, 0, 0]\n",
      "5.973412238120581\n",
      "======\n",
      "k is  0.5\n",
      "n is  4\n",
      "lambda is  [0.6, 0.1, 0.1, 0.1, 0.1]\n",
      "6.183719049935057\n",
      "======\n",
      "k is  0.5\n",
      "n is  4\n",
      "lambda is  [0.6, 0.2, 0.1, 0.1, 0]\n",
      "5.8354340844151915\n",
      "======\n",
      "k is  0.5\n",
      "n is  4\n",
      "lambda is  [0.5, 0.2, 0.2, 0.1, 0]\n",
      "5.904990346805739\n"
     ]
    }
   ],
   "source": [
    "n_list = [1,2,3,4]\n",
    "k_list = [0.01, 0.05, 0.1, 0.5]\n",
    "lamb_dict = dict()\n",
    "lamb_dict[1] = [[1,0], [0.8,0.2], [0.5,0.5], [0.2,0.8]]\n",
    "lamb_dict[2] = [[1,0,0], [0.8,0.1,0.1], [0.5,0.25,0.25], [1/3,1/3,1/3]]\n",
    "lamb_dict[3] = [[1,0,0,0], [0.7,0.1,0.1,0.1], [0.5, 0.5/3, 0.5/3, 0.5/3], [1/4,1/4,1/4,1/4]]\n",
    "lamb_dict[4] = [[1,0,0,0,0], [0.6,0.1,0.1,0.1,0.1], [0.6,0.2,0.1,0.1,0], [0.5,0.2,0.2,0.1,0]]\n",
    "for n in n_list:\n",
    "    for k in k_list:\n",
    "        for lamb_list in lamb_dict[n]:\n",
    "            m = create_ngram_model(NgramModelWithInterpolation, 'shakespeare_input.txt', n, k)\n",
    "            m.set_lambda(lamb_list)\n",
    "            print('======')\n",
    "            print('k is ', k)\n",
    "            print('n is ', n)\n",
    "            print('lambda is ', lamb_list)\n",
    "            with open('shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n",
    "                print(m.perplexity(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "k is  0.0001\n",
      "n is  5\n",
      "lambda is  [0.5, 0.2, 0.2, 0.05, 0.05, 0]\n",
      "5.180455531950972\n",
      "======\n",
      "k is  0.0001\n",
      "n is  5\n",
      "lambda is  [0.4, 0.3, 0.2, 0.05, 0.05, 0]\n",
      "5.171307105538554\n",
      "======\n",
      "k is  0.0001\n",
      "n is  5\n",
      "lambda is  [0.3, 0.3, 0.2, 0.15, 0.05, 0]\n",
      "5.253177336604155\n",
      "======\n",
      "k is  0.0005\n",
      "n is  5\n",
      "lambda is  [0.5, 0.2, 0.2, 0.05, 0.05, 0]\n",
      "5.141533203865685\n",
      "======\n",
      "k is  0.0005\n",
      "n is  5\n",
      "lambda is  [0.4, 0.3, 0.2, 0.05, 0.05, 0]\n",
      "5.132534771174637\n",
      "======\n",
      "k is  0.0005\n",
      "n is  5\n",
      "lambda is  [0.3, 0.3, 0.2, 0.15, 0.05, 0]\n",
      "5.214595687678927\n",
      "======\n",
      "k is  0.01\n",
      "n is  5\n",
      "lambda is  [0.5, 0.2, 0.2, 0.05, 0.05, 0]\n",
      "5.112652121036909\n",
      "======\n",
      "k is  0.01\n",
      "n is  5\n",
      "lambda is  [0.4, 0.3, 0.2, 0.05, 0.05, 0]\n",
      "5.0998365326022626\n",
      "======\n",
      "k is  0.01\n",
      "n is  5\n",
      "lambda is  [0.3, 0.3, 0.2, 0.15, 0.05, 0]\n",
      "5.180063861930196\n"
     ]
    }
   ],
   "source": [
    "from skeleton.hw2_skeleton_char import ngrams,NgramModel,create_ngram_model,NgramModelWithInterpolation\n",
    "import random\n",
    "n_list = [5]\n",
    "k_list = [0.0001, 0.0005, 0.01]\n",
    "lamb_dict = dict()\n",
    "lamb_dict[5] = [[0.5,0.2,0.2,0.05,0.05,0], [0.4, 0.3,0.2,0.05,0.05,0] , [0.3,0.3,0.2,0.15,0.05,0]]\n",
    "for n in n_list:\n",
    "    for k in k_list:\n",
    "        for lamb_list in lamb_dict[n]:\n",
    "            m = create_ngram_model(NgramModelWithInterpolation, 'shakespeare_input.txt', n, k)\n",
    "            m.set_lambda(lamb_list)\n",
    "            print('======')\n",
    "            print('k is ', k)\n",
    "            print('n is ', n)\n",
    "            print('lambda is ', lamb_list)\n",
    "            with open('shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n",
    "                print(m.perplexity(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "k is  0.01\n",
      "n is  6\n",
      "lambda is  [0.3, 0.2, 0.15, 0.15, 0.1, 0.1, 0]\n",
      "5.249738443251137\n",
      "======\n",
      "k is  0.01\n",
      "n is  6\n",
      "lambda is  [0.4, 0.3, 0.1, 0.1, 0.05, 0.05, 0]\n",
      "5.252540350332047\n",
      "======\n",
      "k is  0.05\n",
      "n is  6\n",
      "lambda is  [0.3, 0.2, 0.15, 0.15, 0.1, 0.1, 0]\n",
      "5.491573482348842\n",
      "======\n",
      "k is  0.05\n",
      "n is  6\n",
      "lambda is  [0.4, 0.3, 0.1, 0.1, 0.05, 0.05, 0]\n",
      "5.544356447189646\n",
      "======\n",
      "k is  0.1\n",
      "n is  6\n",
      "lambda is  [0.3, 0.2, 0.15, 0.15, 0.1, 0.1, 0]\n",
      "5.71220073819207\n",
      "======\n",
      "k is  0.1\n",
      "n is  6\n",
      "lambda is  [0.4, 0.3, 0.1, 0.1, 0.05, 0.05, 0]\n",
      "5.822176043580539\n",
      "======\n",
      "k is  0.5\n",
      "n is  6\n",
      "lambda is  [0.3, 0.2, 0.15, 0.15, 0.1, 0.1, 0]\n",
      "6.658200394395542\n",
      "======\n",
      "k is  0.5\n",
      "n is  6\n",
      "lambda is  [0.4, 0.3, 0.1, 0.1, 0.05, 0.05, 0]\n",
      "7.073804333274236\n"
     ]
    }
   ],
   "source": [
    "n_list = [6]\n",
    "k_list = [0.01, 0.05, 0.1, 0.5]\n",
    "lamb_dict = dict()\n",
    "lamb_dict[6] = [[0.3, 0.2,0.15,0.15,0.1,0.1,0], [0.4, 0.3,0.1, 0.1,0.05,0.05,0]]\n",
    "for n in n_list:\n",
    "    for k in k_list:\n",
    "        for lamb_list in lamb_dict[n]:\n",
    "            m = create_ngram_model(NgramModelWithInterpolation, 'shakespeare_input.txt', n, k)\n",
    "            m.set_lambda(lamb_list)\n",
    "            print('======')\n",
    "            print('k is ', k)\n",
    "            print('n is ', n)\n",
    "            print('lambda is ', lamb_list)\n",
    "            with open('shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n",
    "                print(m.perplexity(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your report, experiment with a few different lambdas and values of k and discuss their effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level N-gram Language Models [Required for CS 7650. Bonus for CS 4650] [10 pts]\n",
    "\n",
    "You should complete functions in the script `hw2_skeleton_word.py` in this part. After submitting `hw2_skeleton_word.py` to Gradescope and passing all the test cases for this part, you can get full score. Instructions are similar to the instructions above. It is convenient to first use `text.strip().split()` to convert a string of word sequence to a list of words. In some functions, we provide `text.strip().split()`. You can use it optionally. \n",
    "\n",
    "Besides the corpus above, we also provide you [training data for word-level langauge models](https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/programming/HW2/train_e.zip) and [dev data for word-level langauge models](https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/programming/HW2/val_e.zip), in which each sentence has been processed with word tokenizer and `[EOS]` token has been appended to the end of each sentences.  `[EOS]` can be regarded as the sentence boundary when generating a paragraph or evaluating the perplexity of a paragraph.\n",
    "\n",
    "\n",
    "## Part 1: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skeleton.hw2_skeleton_word import ngrams,NgramModel,create_ngram_model,NgramModelWithInterpolation\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('~ ~', 'I'),\n",
       " ('~ I', 'love'),\n",
       " ('I love', 'Natural'),\n",
       " ('love Natural', 'Language'),\n",
       " ('Natural Language', 'Processing')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(2, 'I love Natural Language Processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = NgramModel(1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i', 'language', 'love', 'natural', 'processing'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.update('I love natural language processing')\n",
    "m.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'~': 2, 'i': 2, 'love': 2, 'natural': 1, 'language': 1, 'machine': 1}\n",
      "===\n",
      "{'~': {'i': 2}, 'i': {'love': 2}, 'love': {'natural': 1, 'machine': 1}, 'natural': {'language': 1}, 'language': {'processing': 1}, 'machine': {'learning': 1}}\n"
     ]
    }
   ],
   "source": [
    "m.update('I love machine learning')\n",
    "m.get_vocab()\n",
    "print(m.ngrams)\n",
    "print('===')\n",
    "print(m.ngrams_char_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.prob('I', 'love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037037037037037035"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.prob('~', 'You')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40740740740740744"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.prob('love', 'natural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.update('You love computer vision')\n",
    "m.update('I was late today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n",
      "['computer', 'i', 'language', 'late', 'learning', 'love', 'machine', 'natural', 'processing', 'today', 'vision', 'was', 'you']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'you',\n",
       " 'vision',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'learning',\n",
       " 'was',\n",
       " 'i',\n",
       " 'i',\n",
       " 'you',\n",
       " 'i',\n",
       " 'vision',\n",
       " 'computer',\n",
       " 'i',\n",
       " 'processing',\n",
       " 'i',\n",
       " 'you',\n",
       " 'you',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'you',\n",
       " 'i',\n",
       " 'i']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1)\n",
    "[m.random_word('~') for i in range(25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n",
      "['are', 'friends', 'we', 'welcome', 'you']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'~ love'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = NgramModel(1, 0)\n",
    "m.update('You are welcome')\n",
    "m.update('We are friends')\n",
    "random.seed(1)\n",
    "m.random_text(25)\n",
    "''.join(' '.join(['~'] * 2).split(' ')[1:]) + ' love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the psychodynamic therapy truism , mothers of only 12 years . [E0S] It was a `` level the scores level and point guard to find out for `` travel expenses . [E0S] Dressed in their area , '' Kiraly said . [E0S] 1am : 'Female victim : the company released a report from a manhole cover , where we have questions for U.S. forces in Wales [E0S] Levels of acetone , and cared for thousands of former students , especially now that her research on children 's favourite recipe had to employ staff who started the second embarrassing discovery late in the autumn . [E0S] Any local council has said virtual reality , go on strike in their tricky relationship . [E0S] But while Mel McLaughlin . [E0S] I think that after a furious response , he noted the `` great nation , and Zita , two ; with the step up all their Christmases were ruined by Vanessa 's little sister . [E0S] As the years [E0S] Selfie : But let 's rest and make socialising the focus is a problem and not let Corbyn 'experiment ' with her partner Daniel French , both times because of Trident nuclear weapons are kept in the Vilnius tourist office ( 4 ) = 2016 [E0S] And there were huge queues to try to portray the legendary filmmaker has not gone into the four-day competition though it hurt a hair transplant [E0S] UN Special Envoy to Syria . [E0S] PGA and\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'train_e.txt', 2)\n",
    "m.random_text(250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the immediate term , though , with Horlin-Smith beating the brothers and uploading them to YouTube . [E0S] Yet many researchers show us that this `` most virtuous and wise '' son of the Republican candidates are all promising they would do otherwise . [E0S] Another couple celebrating graduation would certainly make a memorable couple if their faces were true to life [E0S] The porcelain girl has been missing from north London said she did not show up at the mercy of shysters in places you do n't want to hear from the president , `` Do I not hate those who hate the West and its allies are inciting proliferation and giving moral and legal cover to the likes of Donald Trump . [E0S] With a record jackpot on offer , `` they would be reluctant to negotiate anything major with a lame duck could shovel a lot of talk about the next iPhone is released , according to the criminal complaint said . [E0S] The scoop in this image taken on June 23 . [E0S] Here 's what you 're having children '' [E0S] I think ( the Challenger crew were killed when an Indonesia AirAsia plane en route to the scene which was littered with posters and leaflets carrying images of the same name three years ago , he says , emphasis on the D. He adopts a deep , painful truth about my experience , is measured . [E0S] Prof Verplanken added : 'Timing environmental\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'train_e.txt', 3)\n",
    "m.random_text(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the immediate term , though , that lurk in the pitch , particularly against the new ball . [E0S] Oscar nominee Ellen Page ( `` Juno '' ) can be seen warning that the West will eventually turn on its Muslim population , before Trump is heard calling for the ban [E0S] The front-runner is Democrat Joaquin Arambula , an emergency room physician whose father once was an assemblyman . [E0S] Popeye is a popular figure with the players and apparently he and his wife Kim do not know the sex of their baby but have revealed that it is due in July . [E0S] And what 's the number one life lesson you want people , when they sing , they do n't care how much noise they make . [E0S] Now Playing : 'GMA ' Ultimate Tailgating Challenge ' : Lara 's Nittany Nachos [E0S] Suu Kyi 's party prepares to take charge of Myanmar parliament [E0S] The Nerang 'Loch Ness monster '' snake.Source : Supplied [E0S] A soldier from India 's counter-terrorism force , the National Security Guard ( NSG ) led the fight against the militants , according to top security officials . [E0S] Apple has plenty of repeat business - once hooked in , data shows people are far more likely to endure . [E0S] The MV Tatoosh , a 300ft yacht owned by the billionaire Allen , ripped up 14,000 square feet of coral reef in the West Bay replenishment zone , according\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'train_e.txt', 4)\n",
    "m.random_text(250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think these outputs are more reasonable than character-level language models?\n",
    "\n",
    "After generating a bunch of short passages, do you notice anything? *They all start with In!*  Why is that? Is the model trying to be clever? First, generate the word *In*. Explain what is going on in your writeup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Perplexity, Smoothing, and Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0409040291494223"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skeleton.hw2_skeleton_word import ngrams,NgramModel,create_ngram_model,NgramModelWithInterpolation\n",
    "import random\n",
    "m = NgramModel(1, 0.1)\n",
    "m.update('I love natural language processing')\n",
    "m.update('You love machine learning')\n",
    "m.perplexity('I love machine learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.885785532341627"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.perplexity('I love python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30617.796152024268\n"
     ]
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'train_e.txt', 2, k=0.1)\n",
    "with open('val_e.txt', encoding='utf-8', errors='ignore') as f:\n",
    "    print(m.perplexity(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.743416490252569"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = NgramModel(1, 1)\n",
    "m.update('I love natural language processing')\n",
    "m.update('You love machine learning')\n",
    "m.perplexity('I love machine learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0822019955734"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.perplexity('I love python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62983\n",
      "51817.86687395116\n"
     ]
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 2, k=0.1)\n",
    "print(len(m.get_vocab()))\n",
    "with open('shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n",
    "    print(m.perplexity(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129555\n",
      "30617.796152024268\n"
     ]
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'train_e.txt', 2, k=0.1)\n",
    "print(len(m.get_vocab()))\n",
    "with open('val_e.txt', encoding='utf-8', errors='ignore') as f:\n",
    "    print(m.perplexity(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.5, 0.1111111111111111]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20370370370370372"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skeleton.hw2_skeleton_word import ngrams,NgramModel,create_ngram_model,NgramModelWithInterpolation\n",
    "import random\n",
    "m = NgramModelWithInterpolation(2, 0)\n",
    "m.update('I love natural language processing')\n",
    "m.update('You love machine learning')\n",
    "m.prob('I love','machine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, 0.1111111111111111]\n",
      "[1.0, 1.0, 0.2222222222222222]\n",
      "[0.0, 0.5, 0.1111111111111111]\n",
      "[1.0, 1.0, 0.1111111111111111]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'acc', 'ba']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.perplexity('I love machine learning')\n",
    "a = {'acc', 'ba','a'}\n",
    "sorted(list(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16623093681917211"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = NgramModelWithInterpolation(2, 1)\n",
    "m.update('I love natural language processing')\n",
    "m.update('You love machine learning')\n",
    "m.prob('~ I','love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.76855019085475"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.perplexity('I love machine learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "k is  0.01\n",
      "n is  1\n",
      "lambda is  [0.6, 0.4]\n",
      "3181.708038897317\n",
      "======\n",
      "k is  0.01\n",
      "n is  1\n",
      "lambda is  [0.5, 0.5]\n",
      "3079.893266034102\n",
      "======\n",
      "k is  0.01\n",
      "n is  1\n",
      "lambda is  [0.4, 0.6]\n",
      "3050.561110210501\n",
      "======\n",
      "k is  0.001\n",
      "n is  1\n",
      "lambda is  [0.6, 0.4]\n",
      "3184.4211792037318\n",
      "======\n",
      "k is  0.001\n",
      "n is  1\n",
      "lambda is  [0.5, 0.5]\n",
      "3090.8575120741034\n",
      "======\n",
      "k is  0.001\n",
      "n is  1\n",
      "lambda is  [0.4, 0.6]\n",
      "3077.52730372507\n",
      "======\n",
      "k is  0.005\n",
      "n is  1\n",
      "lambda is  [0.6, 0.4]\n",
      "3130.2175852619416\n",
      "======\n",
      "k is  0.005\n",
      "n is  1\n",
      "lambda is  [0.5, 0.5]\n",
      "3037.3760614840326\n",
      "======\n",
      "k is  0.005\n",
      "n is  1\n",
      "lambda is  [0.4, 0.6]\n",
      "3017.7192982324673\n"
     ]
    }
   ],
   "source": [
    "from skeleton.hw2_skeleton_word import ngrams,NgramModel,create_ngram_model,NgramModelWithInterpolation\n",
    "import random\n",
    "n_list = [1]\n",
    "k_list = [0.01, 0.001, 0.005]\n",
    "lamb_dict = dict()\n",
    "lamb_dict[1] = [[0.6,0.4], [0.5,0.5], [0.4,0.6]]\n",
    "# lamb_dict[2] = [[1,0,0], [0.8,0.1,0.1], [0.5,0.25,0.25], [1/3,1/3,1/3]]\n",
    "# lamb_dict[3] = [[1,0,0,0], [0.7,0.1,0.1,0.1], [0.5, 0.5/3, 0.5/3, 0.5/3], [1/4,1/4,1/4,1/4]]\n",
    "# lamb_dict[4] = [[1,0,0,0,0], [0.6,0.1,0.1,0.1,0.1], [0.6,0.2,0.1,0.1,0], [0.5,0.2,0.2,0.1,0]]\n",
    "for n in n_list:\n",
    "    for k in k_list:\n",
    "        for lamb_list in lamb_dict[n]:\n",
    "            m = create_ngram_model(NgramModelWithInterpolation, 'shakespeare_input.txt', n, k)\n",
    "            m.set_lambda(lamb_list)\n",
    "            print('======')\n",
    "            print('k is ', k)\n",
    "            print('n is ', n)\n",
    "            print('lambda is ', lamb_list)\n",
    "            with open('shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n",
    "                print(m.perplexity(f.read()))\n",
    "# m = create_ngram_model(NgramModelWithInterpolation, 'shakespeare_input.txt', 3, k=0.5)\n",
    "# m.lamb=[0.3,0.2,0.2,0.3]\n",
    "# print(len(m.get_vocab()))\n",
    "# with open('shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n",
    "#     print(m.perplexity(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following code could take about 10 minutes. This should be finished within 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "k is  0.01\n",
      "n is  1\n",
      "lambda is  [0.6, 0.4]\n",
      "1206.7958424364301\n",
      "======\n",
      "k is  0.01\n",
      "n is  1\n",
      "lambda is  [0.5, 0.5]\n",
      "1240.0032530939195\n",
      "======\n",
      "k is  0.01\n",
      "n is  1\n",
      "lambda is  [0.4, 0.6]\n",
      "1302.0066884501048\n",
      "======\n",
      "k is  0.001\n",
      "n is  1\n",
      "lambda is  [0.6, 0.4]\n",
      "1030.2952286966201\n",
      "======\n",
      "k is  0.001\n",
      "n is  1\n",
      "lambda is  [0.5, 0.5]\n",
      "1077.0983233651714\n",
      "======\n",
      "k is  0.001\n",
      "n is  1\n",
      "lambda is  [0.4, 0.6]\n",
      "1152.302276439595\n",
      "======\n",
      "k is  0.005\n",
      "n is  1\n",
      "lambda is  [0.6, 0.4]\n",
      "1128.5045820959924\n",
      "======\n",
      "k is  0.005\n",
      "n is  1\n",
      "lambda is  [0.5, 0.5]\n",
      "1167.8842227570824\n",
      "======\n",
      "k is  0.005\n",
      "n is  1\n",
      "lambda is  [0.4, 0.6]\n",
      "1235.489752926682\n",
      "======\n",
      "k is  0.01\n",
      "n is  2\n",
      "lambda is  [0.8, 0.1, 0.1]\n",
      "2141.162987841561\n",
      "======\n",
      "k is  0.01\n",
      "n is  2\n",
      "lambda is  [0.5, 0.25, 0.25]\n",
      "1356.8097030804897\n",
      "======\n",
      "k is  0.01\n",
      "n is  2\n",
      "lambda is  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "1196.313234737732\n",
      "======\n",
      "k is  0.01\n",
      "n is  2\n",
      "lambda is  [0.2, 0.5, 0.3]\n",
      "1112.0706515982292\n",
      "======\n",
      "k is  0.001\n",
      "n is  2\n",
      "lambda is  [0.8, 0.1, 0.1]\n",
      "1322.4320886823218\n",
      "======\n",
      "k is  0.001\n",
      "n is  2\n",
      "lambda is  [0.5, 0.25, 0.25]\n",
      "910.0814321922466\n",
      "======\n",
      "k is  0.001\n",
      "n is  2\n",
      "lambda is  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "836.4863466922613\n",
      "======\n",
      "k is  0.001\n",
      "n is  2\n",
      "lambda is  [0.2, 0.5, 0.3]\n",
      "794.4552827783702\n",
      "======\n",
      "k is  0.005\n",
      "n is  2\n",
      "lambda is  [0.8, 0.1, 0.1]\n",
      "1816.2083113684157\n",
      "======\n",
      "k is  0.005\n",
      "n is  2\n",
      "lambda is  [0.5, 0.25, 0.25]\n",
      "1182.5585661328125\n",
      "======\n",
      "k is  0.005\n",
      "n is  2\n",
      "lambda is  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "1056.2297148282557\n",
      "======\n",
      "k is  0.005\n",
      "n is  2\n",
      "lambda is  [0.2, 0.5, 0.3]\n",
      "985.7428034455099\n"
     ]
    }
   ],
   "source": [
    "from skeleton.hw2_skeleton_word import ngrams,NgramModel,create_ngram_model,NgramModelWithInterpolation\n",
    "import random\n",
    "n_list = [1,2]\n",
    "# k_list = [0.01, 0.05, 0.1, 0.5]\n",
    "lamb_dict = dict()\n",
    "# lamb_dict[1] = [[1,0], [0.8,0.2], [0.5,0.5], [0.2,0.8]]\n",
    "lamb_dict[2] = [[0.8,0.1,0.1], [0.5,0.25,0.25], [1/3,1/3,1/3], [0.2, 0.5,0.3]]\n",
    "# lamb_dict[3] = [[1,0,0,0], [0.7,0.1,0.1,0.1], [0.5, 0.5/3, 0.5/3, 0.5/3], [1/4,1/4,1/4,1/4]]\n",
    "# lamb_dict[4] = [[1,0,0,0,0], [0.6,0.1,0.1,0.1,0.1], [0.6,0.2,0.1,0.1,0], [0.5,0.2,0.2,0.1,0]]\n",
    "# n_list = [1]\n",
    "k_list = [0.01, 0.001, 0.005]\n",
    "# lamb_dict = dict()\n",
    "lamb_dict[1] = [[0.6,0.4], [0.5,0.5], [0.4,0.6]]\n",
    "for n in n_list:\n",
    "    for k in k_list:\n",
    "        for lamb_list in lamb_dict[n]:\n",
    "            m = create_ngram_model(NgramModelWithInterpolation, 'train_e.txt', n, k)\n",
    "            m.set_lambda(lamb_list)\n",
    "            print('======')\n",
    "            print('k is ', k)\n",
    "            print('n is ', n)\n",
    "            print('lambda is ', lamb_list)\n",
    "            with open('val_e.txt', encoding='utf-8', errors='ignore') as f:\n",
    "                print(m.perplexity(f.read()))\n",
    "# m = create_ngram_model(NgramModelWithInterpolation, 'train_e.txt', 2, k=0.1)\n",
    "# with open('val_e.txt', encoding='utf-8', errors='ignore') as f:\n",
    "#     print(m.perplexity(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please compare the perplexity of `shakespeare_sonnets.txt` when using word-level language model and character-level language model. In your writeup, explain why they are different ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aknowledgement:\n",
    "\n",
    "This assigment is adapted from [Chris Callison-Burch's course CIS 530 - Computational Linguistics](http://computational-linguistics-class.org/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
