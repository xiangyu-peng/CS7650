{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rqxpid8J3_xt"
   },
   "source": [
    "# NLP Homework 4 Programming Assignment\n",
    "\n",
    "In this assignment, we will train and evaluate a neural model to tag the parts of speech in a sentence.\n",
    "We will also implement several improvements to the model to test its performance.\n",
    "\n",
    "We will be using English text from the Wall Street Journal, marked with POS tags such as `NNP` (proper noun) and `DT` (determiner).\n",
    "\n",
    "## Building a POS Tagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3X367eCR3_x0"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtnGNDoA3_x3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwA2y6OR3_yE"
   },
   "source": [
    "### Preparing Data\n",
    "\n",
    "The relevant data is present in the files `train.txt` and `test.txt`.\n",
    "\n",
    "`train.txt`: The training data is present in this file. The file contains sequences of words and their respective tags. The data is split into 80% training and 20% development to train the model and tune the hyperparameters, respectively. See `load_tag_data` for details on how to read the training data.\n",
    "\n",
    "`test.txt`: The test data is present in the file. Use this file only for final evaluation of the trained models. See `load_txt_data` for details on how to read the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "kFpH2P1A3_yG",
    "outputId": "1c889944-290e-4231-9b34-8c07f777aaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:  7148\n",
      "Val Data:  1788\n",
      "Test Data:  2012\n",
      "Total tags:  44\n",
      "{'VBD', 'PDT', 'PRP$', 'VB', ',', 'NNPS', 'WP$', ')', 'RP', 'POS', 'JJS', 'CD', ':', 'CC', '$', 'MD', 'NNP', 'DT', 'FW', 'WDT', 'VBG', 'RBR', 'WRB', 'IN', 'VBN', 'EX', 'VBZ', '#', \"''\", 'NNS', '``', '(', 'VBP', 'RBS', 'SYM', 'NN', 'RB', 'UH', 'JJ', 'WP', 'TO', '.', 'PRP', 'JJR'}\n"
     ]
    }
   ],
   "source": [
    "def load_tag_data(tag_file):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "    sent = []\n",
    "    tags = []\n",
    "    with open(tag_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                all_sentences.append(sent)\n",
    "                all_tags.append(tags)\n",
    "                sent = []\n",
    "                tags = []\n",
    "            else:\n",
    "                word, tag, _ = line.strip().split()\n",
    "                sent.append(word)\n",
    "                tags.append(tag)\n",
    "    return all_sentences, all_tags\n",
    "\n",
    "def load_txt_data(txt_file):\n",
    "    all_sentences = []\n",
    "    sent = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if(line.strip() == \"\"):\n",
    "                all_sentences.append(sent)\n",
    "                sent = []\n",
    "            else:\n",
    "                word = line.strip()\n",
    "                sent.append(word)\n",
    "    return all_sentences\n",
    "\n",
    "train_sentences, train_tags = load_tag_data('train.txt')\n",
    "test_sentences = load_txt_data('test.txt')\n",
    "\n",
    "unique_tags = set([tag for tag_seq in train_tags for tag in tag_seq])\n",
    "\n",
    "# Create train-val split from train data\n",
    "train_val_data = list(zip(train_sentences, train_tags))\n",
    "random.shuffle(train_val_data)\n",
    "split = int(0.8 * len(train_val_data))\n",
    "training_data = train_val_data[:split]\n",
    "val_data = train_val_data[split:]\n",
    "\n",
    "print(\"Train Data: \", len(training_data))\n",
    "print(\"Val Data: \", len(val_data))\n",
    "print(\"Test Data: \", len(test_sentences))\n",
    "print(\"Total tags: \", len(unique_tags))\n",
    "print(unique_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlfliN0J-RzV"
   },
   "source": [
    "### Word-to-Index and Tag-to-Index mapping\n",
    "In order to work with text in Tensor format, we need to map each word to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "uojEDun83_yP",
    "outputId": "fb218599-7c4b-4b67-cf4d-929e2e8ce2d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tags 44\n",
      "Vocab size 21589\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "            \n",
    "tag_to_idx = {}\n",
    "for tag in unique_tags:\n",
    "    if tag not in tag_to_idx:\n",
    "        tag_to_idx[tag] = len(tag_to_idx)\n",
    "\n",
    "idx_to_tag = {}\n",
    "for tag in tag_to_idx:\n",
    "    idx_to_tag[tag_to_idx[tag]] = tag\n",
    "\n",
    "print(\"Total tags\", len(tag_to_idx))\n",
    "print(\"Vocab size\", len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H26dqorp3_yX"
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(sent, idx_mapping):\n",
    "    idxs = [idx_mapping[word] for word in sent]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRnBTCwD3_yc"
   },
   "source": [
    "### Set up model\n",
    "We will build and train a Basic POS Tagger which is an LSTM model to tag the parts of speech in a given sentence.\n",
    "\n",
    "\n",
    "First we need to define some default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2P5SHabu3_yf"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 3\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 30\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "TAGSET_SIZE = len(tag_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkkS4oEb3_yk"
   },
   "source": [
    "### Define Model\n",
    "\n",
    "The model takes as input a sentence as a tensor in the index space. This sentence is then converted to embedding space where each word maps to its word embedding. The word embeddings is learned as part of the model training process. \n",
    "\n",
    "These word embeddings act as input to the LSTM which produces a hidden state. This hidden state is then passed to a Linear layer that produces the probability distribution for the tags of every word. The model will output the tag with the highest probability for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCa30HQb3_ym"
   },
   "outputs": [],
   "source": [
    "class BasicPOSTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, dropout=0):\n",
    "        super(BasicPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # an LSTM layer: that takes word embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)        \n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "#         print('tagset_size', tagset_size)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence as the argument, \n",
    "        # compute the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        embedded = self.dropout(self.embedding(sentence))\n",
    "        embedded = embedded.reshape(embedded.shape[0], 1, self.embedding_dim)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        tag_scores = self.fc(outputs.squeeze(0))\n",
    "#         print('output shape =>', tag_scores.shape)\n",
    "#         print('sen shape =>', sentence.shape)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ot9J3MrB3_ys"
   },
   "source": [
    "### Training\n",
    "\n",
    "We define train and evaluate procedures that allow us to train our model using our created train-val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWMGxh4Z3_yv"
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_function, optimizer):\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences. Find the gradient with respect to the loss and update the\n",
    "        # model parameters using the optimizer.\n",
    "        #############################################################################       \n",
    "        tags = prepare_sequence(tags, tag_to_idx)\n",
    "#         print('tags', tags.shape, tags)\n",
    "        sentence = prepare_sequence(sentence, word_to_idx)\n",
    "        tag_scores = model(sentence)\n",
    "        tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "        \n",
    "#         print('tag_scores', tag_scores)\n",
    "        \n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        model.zero_grad()\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_examples += len(sentence)\n",
    "        \n",
    "#         print(torch.norm(model.embedding.weight))\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate(model, loss_function, optimizer)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate(model, loss_function, optimizer):\n",
    "  # returns:: avg_val_loss (float)\n",
    "  # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            \n",
    "            tag_scores = model(prepare_sequence(sentence, word_to_idx))\n",
    "            tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "            tags = prepare_sequence(tags, tag_to_idx)\n",
    "            \n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            val_loss += loss\n",
    "            \n",
    "            max_preds = tag_scores.argmax(dim = 1, keepdim = True).reshape(1,-1) # get the index of the max probability\n",
    "            tags = tags.reshape(1, -1)\n",
    "            correct += int(torch.sum(max_preds==tags))\n",
    "            val_examples += max_preds.shape[1] \n",
    "            \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsuHjjH1rQeS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30\tAvg Train Loss: 0.1238\tAvg Val Loss: 0.1077\t Val Accuracy: 33\n",
      "Epoch: 2/30\tAvg Train Loss: 0.0892\tAvg Val Loss: 0.0768\t Val Accuracy: 49\n",
      "Epoch: 3/30\tAvg Train Loss: 0.0658\tAvg Val Loss: 0.0605\t Val Accuracy: 60\n",
      "Epoch: 4/30\tAvg Train Loss: 0.0523\tAvg Val Loss: 0.0516\t Val Accuracy: 67\n",
      "Epoch: 5/30\tAvg Train Loss: 0.0445\tAvg Val Loss: 0.0468\t Val Accuracy: 76\n",
      "Epoch: 6/30\tAvg Train Loss: 0.0392\tAvg Val Loss: 0.0429\t Val Accuracy: 79\n",
      "Epoch: 7/30\tAvg Train Loss: 0.0357\tAvg Val Loss: 0.0407\t Val Accuracy: 80\n",
      "Epoch: 8/30\tAvg Train Loss: 0.0331\tAvg Val Loss: 0.0390\t Val Accuracy: 81\n",
      "Epoch: 9/30\tAvg Train Loss: 0.0306\tAvg Val Loss: 0.0372\t Val Accuracy: 81\n",
      "Epoch: 10/30\tAvg Train Loss: 0.0285\tAvg Val Loss: 0.0356\t Val Accuracy: 82\n",
      "Epoch: 11/30\tAvg Train Loss: 0.0266\tAvg Val Loss: 0.0349\t Val Accuracy: 83\n",
      "Epoch: 12/30\tAvg Train Loss: 0.0252\tAvg Val Loss: 0.0335\t Val Accuracy: 84\n",
      "Epoch: 13/30\tAvg Train Loss: 0.0239\tAvg Val Loss: 0.0325\t Val Accuracy: 85\n",
      "Epoch: 14/30\tAvg Train Loss: 0.0227\tAvg Val Loss: 0.0317\t Val Accuracy: 86\n",
      "Epoch: 15/30\tAvg Train Loss: 0.0218\tAvg Val Loss: 0.0315\t Val Accuracy: 86\n",
      "Epoch: 16/30\tAvg Train Loss: 0.0210\tAvg Val Loss: 0.0307\t Val Accuracy: 86\n",
      "Epoch: 17/30\tAvg Train Loss: 0.0202\tAvg Val Loss: 0.0301\t Val Accuracy: 87\n",
      "Epoch: 18/30\tAvg Train Loss: 0.0196\tAvg Val Loss: 0.0296\t Val Accuracy: 87\n",
      "Epoch: 19/30\tAvg Train Loss: 0.0189\tAvg Val Loss: 0.0289\t Val Accuracy: 87\n",
      "Epoch: 20/30\tAvg Train Loss: 0.0182\tAvg Val Loss: 0.0286\t Val Accuracy: 87\n",
      "Epoch: 21/30\tAvg Train Loss: 0.0176\tAvg Val Loss: 0.0280\t Val Accuracy: 88\n",
      "Epoch: 22/30\tAvg Train Loss: 0.0170\tAvg Val Loss: 0.0277\t Val Accuracy: 88\n",
      "Epoch: 23/30\tAvg Train Loss: 0.0165\tAvg Val Loss: 0.0273\t Val Accuracy: 88\n",
      "Epoch: 24/30\tAvg Train Loss: 0.0160\tAvg Val Loss: 0.0268\t Val Accuracy: 88\n",
      "Epoch: 25/30\tAvg Train Loss: 0.0155\tAvg Val Loss: 0.0265\t Val Accuracy: 89\n",
      "Epoch: 26/30\tAvg Train Loss: 0.0149\tAvg Val Loss: 0.0261\t Val Accuracy: 89\n",
      "Epoch: 27/30\tAvg Train Loss: 0.0145\tAvg Val Loss: 0.0258\t Val Accuracy: 89\n",
      "Epoch: 28/30\tAvg Train Loss: 0.0139\tAvg Val Loss: 0.0251\t Val Accuracy: 89\n",
      "Epoch: 29/30\tAvg Train Loss: 0.0134\tAvg Val Loss: 0.0250\t Val Accuracy: 89\n",
      "Epoch: 30/30\tAvg Train Loss: 0.0130\tAvg Val Loss: 0.0247\t Val Accuracy: 89\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "model = BasicPOSTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAGSET_SIZE)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), lr = LEARNING_RATE)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = loss_function.to(device)\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train(epoch, model, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uK6mT_k8NRvB"
   },
   "source": [
    "**Sanity Check!** After 5 epochs you should get around 53% accuracy, after 10 epochs the accuracy should be around 61%, the accuracy rises to around 70% after 20 epochs, and to around 75% accuracy after 30 epochs.\n",
    "\n",
    "**Note1** If you run the notebook on CPU, it could take time to run all 30 epochs. So try the small number of epochs first to sanity check and then run the model for all 30 epochs. You're free to use GPU for this assignment.\n",
    "\n",
    "**Note2** Your accuracy may not match exactly to what is reported here but as long as the trend is increasing, it should be good.\n",
    "\n",
    "**Note3** The reported numbers are with the default hyperparameters. If you reach the desired accuracy, you can try different hyperparameter settings to improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a method to save the predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2e9aoWNcNomd"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    predicted_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentence in test_sentences:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the test loop\n",
    "            # This method saves the predicted tags for the sentences in the test set.\n",
    "            # The tags are first added to a list which is then written to a file for\n",
    "            # submission. An empty string is added after every sequence of tags\n",
    "            # corresponding to a sentence to add a newline following file formatting\n",
    "            # convention, as has been done already.\n",
    "            #############################################################################\n",
    "            tag_scores = model(prepare_sequence(sentence, word_to_idx))\n",
    "            tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "            max_preds = tag_scores.argmax(dim = 1, keepdim = True)\n",
    "            for tag in max_preds:\n",
    "                tag = int(tag)\n",
    "                predicted_tags.append(idx_to_tag[tag])           \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            predicted_tags.append(\"\")\n",
    "\n",
    "    with open('test_labels.txt', 'w+') as f:\n",
    "        for item in predicted_tags:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azW08GfZSHcQ"
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9T6s3XTFG46"
   },
   "source": [
    "\n",
    "### Test accuracy\n",
    "\n",
    "Evaluate your performance on the test data by submitting test_labels.txt generated by the method above and **report your test accuracy here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** is **87.445%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iP64WDReBuDr"
   },
   "source": [
    "Imitate the above method to generate prediction for validation data.\n",
    "Create lists of words, tags predicted by the model and ground truth tags. \n",
    "\n",
    "Use these lists to carry out error analysis to find the top-10 types of errors made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QgMHr7HCn1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_tag\t| model_tag\t| freq.\t| examples\n",
      "-------------------------------------------------------------------------\n",
      "VBP\t| VB\t\t| 256\t| ['say', 'expect', 'say', 'lose', 'rely']\n",
      "-------------------------------------------------------------------------\n",
      "VBN\t| VBD\t\t| 235\t| ['signed', 'failed', 'produced', 'considered', 'measured']\n",
      "-------------------------------------------------------------------------\n",
      "NNP\t| JJ\t\t| 190\t| ['Peoria', 'G.D.', 'Pot', 'Hollister', 'London']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| NNP\t\t| 182\t| ['Return', 'medical-airlift', 'craft', 'laughingstock', 'minimun']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| JJ\t\t| 178\t| ['fixed-income', 'pad', 'manuevering', 'symptom', 'ranch']\n",
      "-------------------------------------------------------------------------\n",
      "POS\t| VBZ\t\t| 169\t| [\"'s\", \"'s\", \"'s\", \"'s\", \"'s\"]\n",
      "-------------------------------------------------------------------------\n",
      "JJ\t| NNP\t\t| 150\t| ['Cautious', 'Gargantuan', 'trivial', 'die-hard', 'Durable']\n",
      "-------------------------------------------------------------------------\n",
      "VBD\t| VBN\t\t| 131\t| ['made', 'disclosed', 'threatened', 'applied', 'sold']\n",
      "-------------------------------------------------------------------------\n",
      "MD\t| VBZ\t\t| 124\t| ['can', 'can', 'should', \"'ll\", 'can']\n",
      "-------------------------------------------------------------------------\n",
      "VB\t| NN\t\t| 104\t| ['buy-back', 'SHAKE', 'wind', 'gain', 'rise']\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions from val data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions(model, test_sentences):\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    # Your code here\n",
    "    word_list, model_tags, gt_tags = [], [], []\n",
    "    for sentence, tags in test_sentences:\n",
    "        tag_scores = model(prepare_sequence(sentence, word_to_idx))\n",
    "        tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "\n",
    "        max_preds = tag_scores.argmax(dim = 1, keepdim = True).reshape(1,-1) # get the index of the max probability\n",
    "        model_tags += [idx_to_tag[idx] for idx in max_preds.tolist()[0]]   \n",
    "        word_list += sentence\n",
    "        gt_tags += tags\n",
    "    \n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    # Your code here\n",
    "    \n",
    "    error_dict = dict()\n",
    "    for idx, word in enumerate(word_list):\n",
    "        if model_tags[idx] != gt_tags[idx]:\n",
    "            tuple_error = (model_tags[idx],gt_tags[idx])\n",
    "            if tuple_error in error_dict:\n",
    "                error_dict[tuple_error] += [word]\n",
    "            else:\n",
    "                error_dict[tuple_error] = [word]\n",
    "    \n",
    "    return sorted(error_dict.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "word_list, model_tags, gt_tags = generate_predictions(model, val_data)\n",
    "print(\"gt_tag\\t| model_tag\\t| freq.\\t| examples\")\n",
    "horizontal_line = \"-------------------------------------------------------------------------\"\n",
    "errors = error_analysis(word_list, model_tags, gt_tags)[:10]\n",
    "print(horizontal_line)\n",
    "for error in errors:\n",
    "    tags, examples = error\n",
    "    model_tag, gt_tag = tags\n",
    "    num_errors, example_words = len(examples), examples\n",
    "    print(\"{}\\t| {}\\t\\t| {}\\t| {}\\n{}\".format(gt_tag, model_tag, num_errors, random.sample(example_words, 5), horizontal_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRNjFRDcD2h7"
   },
   "source": [
    "### Error analysis\n",
    "**Report your findings here.**  \n",
    "What kinds of errors did the model make and why do you think it made them?\n",
    "\n",
    "Answer:\n",
    "The model is not good at distinguishing different types of verbs.\n",
    "VBD\t| VBN and VBN | VBD can happen b/c many words vbn and vbd are the same.\n",
    "Also for VBP | VB, many verb's vbp and vb are the same.\n",
    "\n",
    "JJ | NNP and NN | JJ is a type of error that we often make in real life. Many words have Ambiguity, and they can be adj and noun, it is hard for model to distinguish them.\n",
    "\n",
    "\n",
    "\n",
    "Also NNP,JJ, NN are confusing model since their position in the sentence often similar.\n",
    "\n",
    "In general, I think the model did a pretty decent job with the POS tagging. Different noun is very easy to be mistaken in this model, because LSTM does not have enough information to distinguish them only according to location. So we need a more accurate way to do POS tagging, we need to include the information of the word itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svXyUssdXZ4r"
   },
   "source": [
    "## II. Character level PoS Tagger\n",
    "\n",
    "Use the character-level information present to augment word embeddings. Words that end with -ing or -ly give quite a bit of information about their POS tags. To incorporate this information, run a character level LSTM on every word (treated as a tensor of characters, each mapped to character-index space) to create a character-level representation of the word. Take the last hidden state from the character level LSTM as the representation and concatenate with the word embedding (as in the WordLSTMPoSTagger) to create a new word embedding that captures more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nX4-3AoxSJeY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 0, '&': 1, 't': 2, 'a': 3, ':': 4, '$': 5, 'B': 6, 'M': 7, 'S': 8, 'T': 9, 'W': 10, 'A': 11, '-': 12, 'i': 13, '2': 14, 'q': 15, 'G': 16, \"'\": 17, '5': 18, 'z': 19, 'x': 20, 'k': 21, 'h': 22, ';': 23, '9': 24, 'f': 25, 'y': 26, 'e': 27, 'F': 28, '#': 29, 'N': 30, 'V': 31, 'Y': 32, '%': 33, '.': 34, 'X': 35, 'n': 36, '7': 37, 'u': 38, ',': 39, 'r': 40, 'Z': 41, '`': 42, '4': 43, 'd': 44, 'J': 45, 'Q': 46, '8': 47, 'g': 48, '!': 49, '1': 50, 'H': 51, 'E': 52, 'O': 53, 'U': 54, 'I': 55, 'l': 56, 'C': 57, '3': 58, '?': 59, 'L': 60, 'R': 61, '0': 62, 'c': 63, '*': 64, 'w': 65, '@': 66, 'j': 67, 'D': 68, '=': 69, 'P': 70, 'K': 71, 'b': 72, 'v': 73, 'm': 74, '6': 75, 's': 76, '\\\\': 77, 'p': 78, '/': 79, ' ': 80}\n",
      "MAX_WORD_LEN 43\n",
      "MAX_WORD_LEN 54\n"
     ]
    }
   ],
   "source": [
    "# Create char to index mapping\n",
    "char_to_idx = {}\n",
    "unique_chars = set()\n",
    "MAX_WORD_LEN = 0\n",
    "\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        for c in word:\n",
    "            unique_chars.add(c)\n",
    "        if len(word) > MAX_WORD_LEN:\n",
    "            MAX_WORD_LEN = len(word)\n",
    "\n",
    "for c in unique_chars:\n",
    "    char_to_idx[c] = len(char_to_idx)\n",
    "char_to_idx[' '] = len(char_to_idx)\n",
    "\n",
    "# New Hyperparameters\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 3\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 30\n",
    "CHAR_EMBEDDING_DIM = 3\n",
    "CHAR_HIDDEN_DIM = 3\n",
    "print(char_to_idx)\n",
    "print('MAX_WORD_LEN', MAX_WORD_LEN)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    for word in sent:\n",
    "        for c in word:\n",
    "            unique_chars.add(c)\n",
    "        if len(word) > MAX_WORD_LEN:\n",
    "            MAX_WORD_LEN = len(word)\n",
    "print('MAX_WORD_LEN', MAX_WORD_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7U0wb4OeOsde"
   },
   "outputs": [],
   "source": [
    "# class CharPOSTagger(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, char_embedding_dim, \n",
    "#                  char_hidden_dim, char_size, vocab_size, tagset_size):\n",
    "#         super(CharPOSTagger, self).__init__()\n",
    "#         #############################################################################\n",
    "#         # TODO: Define and initialize anything needed for the forward pass.\n",
    "#         # You are required to create a model with:\n",
    "#         # an embedding layer: that maps words to the embedding space\n",
    "#         # an char level LSTM: that finds the character level embedding for a word\n",
    "#         # an LSTM layer: that takes the combined embeddings as input and outputs hidden states\n",
    "#         # a Linear layer: maps from hidden state space to tag space\n",
    "#         #############################################################################\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.char_embedding_dim = char_embedding_dim\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "#         self.lstm = nn.LSTM(char_hidden_dim + embedding_dim, hidden_dim)  \n",
    "        \n",
    "#         self.embedding_c = nn.Embedding(MAX_WORD_LEN, char_embedding_dim, padding_idx=1) #char_size + 1, char_embedding_dim\n",
    "#         self.lstm_c = nn.LSTM(char_embedding_dim, char_hidden_dim) \n",
    "        \n",
    "#         self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "#         #############################################################################\n",
    "#         #                             END OF YOUR CODE                              #\n",
    "#         #############################################################################\n",
    "\n",
    "#     def forward(self, sentence, chars):\n",
    "#         tag_scores = None\n",
    "#         #############################################################################\n",
    "#         # TODO: Implement the forward pass.\n",
    "#         # Given a tokenized index-mapped sentence and a character sequence as the arguments, \n",
    "#         # find the corresponding scores for tags\n",
    "#         # returns:: tag_scores (Tensor)\n",
    "#         #############################################################################\n",
    "#         embedded = self.embedding(sentence)\n",
    "# #         print(sentence)\n",
    "# #         chars = torch.LongTensor([char.numpy() for char in chars])\n",
    "#         chars = torch.cat(chars).view(len(chars), 1, -1)\n",
    "    \n",
    "#         embedded_c = self.embedding_c(chars)\n",
    "#         word_char_embeddings = torch.Tensor([])\n",
    "#         for char in embedded_c:\n",
    "#             char = char.view(embedded_c.shape[2], 1, -1)\n",
    "#             outputs_c, _ = self.lstm_c(char)\n",
    "#             word_char_embeddings = torch.cat((word_char_embeddings, outputs_c[-1]), dim = 0)\n",
    "            \n",
    "# #         print('word_char_embeddings', word_char_embeddings.shape, embedded.shape)\n",
    "#         joint_emb = torch.cat((embedded, word_char_embeddings), dim=1)\n",
    "#         joint_emb = joint_emb.reshape(joint_emb.shape[0], 1, -1)\n",
    "        \n",
    "#         outputs, _ = self.lstm(joint_emb)\n",
    "        \n",
    "#         tag_scores = self.fc(outputs.squeeze(0))\n",
    "#         #############################################################################\n",
    "#         #                             END OF YOUR CODE                              #\n",
    "#         #############################################################################\n",
    "#         return tag_scores\n",
    "\n",
    "# def train_char(epoch, model, loss_function, optimizer):\n",
    "#     train_loss = 0\n",
    "#     train_examples = 0\n",
    "#     for sentence, tags in training_data:\n",
    "#         #############################################################################\n",
    "#         # TODO: Implement the training loop\n",
    "#         # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "#         # for sentences as well as character sequences. Find the gradient with \n",
    "#         # respect to the loss and update the model parameters using the optimizer.\n",
    "#         #############################################################################\n",
    "#         tags = prepare_sequence(tags, tag_to_idx)\n",
    "        \n",
    "#         sentence_chars = []\n",
    "#         for w in sentence:\n",
    "#             spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "#             sentence_chars.append(list(spaces + w))\n",
    "#         chars = [prepare_sequence(sentence_char, char_to_idx) for sentence_char in sentence_chars]\n",
    "        \n",
    "#         sentence = prepare_sequence(sentence, word_to_idx)\n",
    "#         tag_scores = model(sentence, chars)\n",
    "        \n",
    "#         tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "        \n",
    "        \n",
    "#         loss = loss_function(tag_scores, tags)\n",
    "#         model.zero_grad()\n",
    "#         loss.backward()        \n",
    "#         optimizer.step()\n",
    "        \n",
    "#         train_loss += loss.item()\n",
    "#         train_examples += len(sentence)\n",
    "#         #############################################################################\n",
    "#         #                             END OF YOUR CODE                              #\n",
    "#         #############################################################################\n",
    "    \n",
    "#     avg_train_loss = train_loss / train_examples\n",
    "#     avg_val_loss, val_accuracy = evaluate_char(model, loss_function, optimizer)\n",
    "        \n",
    "#     print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "#                                                                       EPOCHS, \n",
    "#                                                                       avg_train_loss, \n",
    "#                                                                       avg_val_loss,\n",
    "#                                                                       val_accuracy))\n",
    "\n",
    "# def evaluate_char(model, loss_function, optimizer):\n",
    "#     # returns:: avg_val_loss (float)\n",
    "#     # returns:: val_accuracy (float)\n",
    "#     val_loss = 0\n",
    "#     correct = 0\n",
    "#     val_examples = 0\n",
    "#     with torch.no_grad():\n",
    "#         for sentence, tags in val_data:\n",
    "#             #############################################################################\n",
    "#             # TODO: Implement the evaluate loop\n",
    "#             # Find the average validation loss along with the validation accuracy.\n",
    "#             # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "#             #############################################################################\n",
    "#             sentence_chars = []\n",
    "#             for w in sentence:\n",
    "#                 spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "#                 sentence_chars.append(list(spaces + w))\n",
    "#             chars = [prepare_sequence(sentence_char, char_to_idx) for sentence_char in sentence_chars]\n",
    "        \n",
    "#             tag_scores = model(prepare_sequence(sentence, word_to_idx), chars)\n",
    "#             tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "#             tags = prepare_sequence(tags, tag_to_idx)\n",
    "            \n",
    "#             loss = loss_function(tag_scores, tags)\n",
    "#             val_loss += loss\n",
    "            \n",
    "#             max_preds = tag_scores.argmax(dim = 1, keepdim = True).reshape(1,-1) # get the index of the max probability\n",
    "#             tags = tags.reshape(1, -1)\n",
    "#             correct += int(torch.sum(max_preds==tags))\n",
    "#             val_examples += max_preds.shape[1] \n",
    "#             #############################################################################\n",
    "#             #                             END OF YOUR CODE                              #\n",
    "#             #############################################################################\n",
    "#     val_accuracy = 100. * correct / val_examples\n",
    "#     avg_val_loss = val_loss / val_examples\n",
    "#     return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharPOSTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim, \n",
    "                 char_hidden_dim, char_size, vocab_size, tagset_size):\n",
    "        super(CharPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # an char level LSTM: that finds the character level embedding for a word\n",
    "        # an LSTM layer: that takes the combined embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "        self.lstm = nn.LSTM(char_hidden_dim + embedding_dim, hidden_dim)  \n",
    "        \n",
    "        self.embedding_c = nn.Embedding(char_size + 1, char_embedding_dim, padding_idx=1) #char_size + 1, char_embedding_dim\n",
    "        self.lstm_c = nn.LSTM(162, char_hidden_dim) \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence, chars):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence and a character sequence as the arguments, \n",
    "        # find the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        embedded = self.embedding(sentence)\n",
    "#         print(sentence)\n",
    "#         chars = torch.LongTensor([char.numpy() for char in chars])\n",
    "        chars = torch.cat(chars).view(len(chars), 1, -1)\n",
    "    \n",
    "        embedded_c = self.embedding_c(chars)\n",
    "        embedded_c = embedded_c.view(embedded_c.shape[0], 1, -1)\n",
    "#         print('chars', chars.shape)\n",
    "        outputs_c, _ = self.lstm_c(embedded_c)\n",
    "#         word_char_embeddings = torch.cat((word_char_embeddings, outputs_c[-1]), dim = 0)\n",
    "#         print(outputs_c.shape, embedded.shape)\n",
    "            \n",
    "#         print('word_char_embeddings', word_char_embeddings.shape, embedded.shape)\n",
    "        joint_emb = torch.cat((embedded, outputs_c.reshape(outputs_c.shape[0], -1)), dim=1)\n",
    "        joint_emb = joint_emb.reshape(joint_emb.shape[0], 1, -1)\n",
    "        \n",
    "        outputs, _ = self.lstm(joint_emb)\n",
    "        \n",
    "        tag_scores = self.fc(outputs.squeeze(0))\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores\n",
    "\n",
    "def train_char(epoch, model, loss_function, optimizer):\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences as well as character sequences. Find the gradient with \n",
    "        # respect to the loss and update the model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        tags = prepare_sequence(tags, tag_to_idx)\n",
    "        \n",
    "        sentence_chars = []\n",
    "        for w in sentence:\n",
    "            spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "            sentence_chars.append(list(spaces + w))\n",
    "        chars = [prepare_sequence(sentence_char, char_to_idx) for sentence_char in sentence_chars]\n",
    "        \n",
    "        sentence = prepare_sequence(sentence, word_to_idx)\n",
    "        tag_scores = model(sentence, chars)\n",
    "        \n",
    "        tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "        \n",
    "        \n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        model.zero_grad()\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_examples += len(sentence)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate_char(model, loss_function, optimizer)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate_char(model, loss_function, optimizer):\n",
    "    # returns:: avg_val_loss (float)\n",
    "    # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            sentence_chars = []\n",
    "            for w in sentence:\n",
    "                spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "                sentence_chars.append(list(spaces + w))\n",
    "            chars = [prepare_sequence(sentence_char, char_to_idx) for sentence_char in sentence_chars]\n",
    "        \n",
    "            tag_scores = model(prepare_sequence(sentence, word_to_idx), chars)\n",
    "            tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "            tags = prepare_sequence(tags, tag_to_idx)\n",
    "            \n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            val_loss += loss\n",
    "            \n",
    "            max_preds = tag_scores.argmax(dim = 1, keepdim = True).reshape(1,-1) # get the index of the max probability\n",
    "            tags = tags.reshape(1, -1)\n",
    "            correct += int(torch.sum(max_preds==tags))\n",
    "            val_examples += max_preds.shape[1] \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-QttCw6Otf-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30\tAvg Train Loss: 0.1233\tAvg Val Loss: 0.1011\t Val Accuracy: 34\n",
      "Epoch: 2/30\tAvg Train Loss: 0.0882\tAvg Val Loss: 0.0750\t Val Accuracy: 47\n",
      "Epoch: 3/30\tAvg Train Loss: 0.0657\tAvg Val Loss: 0.0576\t Val Accuracy: 59\n",
      "Epoch: 4/30\tAvg Train Loss: 0.0496\tAvg Val Loss: 0.0424\t Val Accuracy: 77\n",
      "Epoch: 5/30\tAvg Train Loss: 0.0373\tAvg Val Loss: 0.0362\t Val Accuracy: 82\n",
      "Epoch: 6/30\tAvg Train Loss: 0.0317\tAvg Val Loss: 0.0324\t Val Accuracy: 83\n",
      "Epoch: 7/30\tAvg Train Loss: 0.0281\tAvg Val Loss: 0.0298\t Val Accuracy: 85\n",
      "Epoch: 8/30\tAvg Train Loss: 0.0256\tAvg Val Loss: 0.0279\t Val Accuracy: 86\n",
      "Epoch: 9/30\tAvg Train Loss: 0.0236\tAvg Val Loss: 0.0266\t Val Accuracy: 86\n",
      "Epoch: 10/30\tAvg Train Loss: 0.0217\tAvg Val Loss: 0.0251\t Val Accuracy: 88\n",
      "Epoch: 11/30\tAvg Train Loss: 0.0204\tAvg Val Loss: 0.0242\t Val Accuracy: 89\n",
      "Epoch: 12/30\tAvg Train Loss: 0.0192\tAvg Val Loss: 0.0233\t Val Accuracy: 89\n",
      "Epoch: 13/30\tAvg Train Loss: 0.0181\tAvg Val Loss: 0.0223\t Val Accuracy: 90\n",
      "Epoch: 14/30\tAvg Train Loss: 0.0172\tAvg Val Loss: 0.0217\t Val Accuracy: 90\n",
      "Epoch: 15/30\tAvg Train Loss: 0.0165\tAvg Val Loss: 0.0211\t Val Accuracy: 90\n",
      "Epoch: 16/30\tAvg Train Loss: 0.0158\tAvg Val Loss: 0.0207\t Val Accuracy: 90\n",
      "Epoch: 17/30\tAvg Train Loss: 0.0152\tAvg Val Loss: 0.0203\t Val Accuracy: 90\n",
      "Epoch: 18/30\tAvg Train Loss: 0.0147\tAvg Val Loss: 0.0200\t Val Accuracy: 90\n",
      "Epoch: 19/30\tAvg Train Loss: 0.0143\tAvg Val Loss: 0.0196\t Val Accuracy: 90\n",
      "Epoch: 20/30\tAvg Train Loss: 0.0139\tAvg Val Loss: 0.0193\t Val Accuracy: 90\n",
      "Epoch: 21/30\tAvg Train Loss: 0.0137\tAvg Val Loss: 0.0191\t Val Accuracy: 91\n",
      "Epoch: 22/30\tAvg Train Loss: 0.0133\tAvg Val Loss: 0.0189\t Val Accuracy: 91\n",
      "Epoch: 23/30\tAvg Train Loss: 0.0130\tAvg Val Loss: 0.0186\t Val Accuracy: 91\n",
      "Epoch: 24/30\tAvg Train Loss: 0.0127\tAvg Val Loss: 0.0184\t Val Accuracy: 91\n",
      "Epoch: 25/30\tAvg Train Loss: 0.0125\tAvg Val Loss: 0.0182\t Val Accuracy: 91\n",
      "Epoch: 26/30\tAvg Train Loss: 0.0122\tAvg Val Loss: 0.0181\t Val Accuracy: 91\n",
      "Epoch: 27/30\tAvg Train Loss: 0.0120\tAvg Val Loss: 0.0179\t Val Accuracy: 91\n",
      "Epoch: 28/30\tAvg Train Loss: 0.0118\tAvg Val Loss: 0.0177\t Val Accuracy: 91\n",
      "Epoch: 29/30\tAvg Train Loss: 0.0116\tAvg Val Loss: 0.0176\t Val Accuracy: 91\n",
      "Epoch: 30/30\tAvg Train Loss: 0.0114\tAvg Val Loss: 0.0175\t Val Accuracy: 92\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "model = CharPOSTagger(EMBEDDING_DIM, HIDDEN_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, len(char_to_idx), VOCAB_SIZE, TAGSET_SIZE)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), lr = LEARNING_RATE)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = loss_function.to(device)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train_char(epoch, model, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xslNYW8EBKMQ"
   },
   "source": [
    "**Sanity Check!** After 5 epochs you should get around 57% accuracy, after 10 epochs the accuracy should be around 67%, the accuracy rises to around 74% after 20 epochs, and to around 77% accuracy after 30 epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy\n",
    "Also evaluate your performance on the test data by submitting test_labels.txt and **report your test accuracy here**.\n",
    "\n",
    "\n",
    "**89.96%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuLl_BSMeovb"
   },
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Du0raTJreqT2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_tag\t| model_tag\t| freq.\t| examples\n",
      "-------------------------------------------------------------------------\n",
      "VBN\t| VBD\t\t| 196\t| ['built', 'upheld', 'allocated', 'collapsed', 'likened']\n",
      "-------------------------------------------------------------------------\n",
      "IN\t| VBZ\t\t| 196\t| ['that', 'that', 'that', 'that', 'that']\n",
      "-------------------------------------------------------------------------\n",
      "NNP\t| NN\t\t| 189\t| ['Taco', 'Poll', 'Magic', 'Cellular', 'Unity']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| JJ\t\t| 186\t| ['net', 'bank-teller', 'other', 'crude', 'stable']\n",
      "-------------------------------------------------------------------------\n",
      "NNP\t| JJ\t\t| 141\t| ['Gamble', 'Compensation', 'Independent', 'Jacques-Francois', 'Fruehauf']\n",
      "-------------------------------------------------------------------------\n",
      "WDT\t| VBZ\t\t| 124\t| ['that', 'which', 'that', 'that', 'which']\n",
      "-------------------------------------------------------------------------\n",
      "VBD\t| VBN\t\t| 120\t| ['damaged', 'eliminated', 'lent', 'feared', 'threatened']\n",
      "-------------------------------------------------------------------------\n",
      "VB\t| NN\t\t| 118\t| ['train', 'face', 'finance', 'elect', 'help']\n",
      "-------------------------------------------------------------------------\n",
      "JJ\t| NN\t\t| 117\t| ['650-or-so', 'shaky', 'agrarian-reform', 'unaware', 'government-guaranteed']\n",
      "-------------------------------------------------------------------------\n",
      "JJ\t| NNP\t\t| 109\t| ['credit-rating', 'high-powered', 'Western', 'Western', 'Printed']\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions from val data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions_char(model, test_sentences):\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    # Your code here\n",
    "    word_list, model_tags, gt_tags = [], [], []\n",
    "    for sentence, tags in test_sentences:\n",
    "        sentence_chars = []\n",
    "        for w in sentence:\n",
    "            spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "            sentence_chars.append(list(spaces + w))\n",
    "        chars = [prepare_sequence(sentence_char, char_to_idx) for sentence_char in sentence_chars]\n",
    "        tag_scores = model(prepare_sequence(sentence, word_to_idx), chars)\n",
    "        tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "\n",
    "        max_preds = tag_scores.argmax(dim = 1, keepdim = True).reshape(1,-1) # get the index of the max probability\n",
    "        model_tags += [idx_to_tag[idx] for idx in max_preds.tolist()[0]]   \n",
    "        word_list += sentence\n",
    "        gt_tags += tags\n",
    "    \n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis_char(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    # Your code here\n",
    "    error_dict = dict()\n",
    "    for idx, word in enumerate(word_list):\n",
    "        if model_tags[idx] != gt_tags[idx]:\n",
    "            tuple_error = (model_tags[idx],gt_tags[idx])\n",
    "            if tuple_error in error_dict:\n",
    "                error_dict[tuple_error] += [word]\n",
    "            else:\n",
    "                error_dict[tuple_error] = [word]\n",
    "    \n",
    "    return sorted(error_dict.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "word_list, model_tags, gt_tags = generate_predictions_char(model, val_data)\n",
    "print(\"gt_tag\\t| model_tag\\t| freq.\\t| examples\")\n",
    "horizontal_line = \"-------------------------------------------------------------------------\"\n",
    "errors = error_analysis_char(word_list, model_tags, gt_tags)[:10]\n",
    "print(horizontal_line)\n",
    "for error in errors:\n",
    "    tags, examples = error\n",
    "    model_tag, gt_tag = tags\n",
    "    num_errors, example_words = len(examples), examples\n",
    "    print(\"{}\\t| {}\\t\\t| {}\\t| {}\\n{}\".format(gt_tag, model_tag, num_errors, random.sample(example_words, 5), horizontal_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GEP-IgiESzN"
   },
   "source": [
    "\n",
    "**Report your findings here.**  \n",
    "What kinds of errors does the character-level model make as compared to the original model, and why do you think it made them? \n",
    "\n",
    "Compared with part(a), we can still see the model is not easy to deal with types of verbs, but it is better.\n",
    "VBN, VBD is very easy to make mistake because even we consider the word itself, it is still hard to distinguish them since they are the same. Sometimes, the LSTM are not strong enough to give a correct tagging.\n",
    "\n",
    "VB,VBP share the same situation.\n",
    "\n",
    "Also NNP,JJ, NN are confusing model since their position in the sentence often similar.\n",
    "\n",
    "These are all the same trend with part (a).\n",
    "\n",
    "But we can see the confusion of different verbs has been improved.\n",
    "\n",
    "NNP|JJ error is less because we consider chars and words at the same time now. This word is JJ, but in content, it is NNP, so when we consider more information, the model is relatively easier to tag it correctly.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQdc3gH8d_a4"
   },
   "source": [
    "## Modifications\n",
    "\n",
    "Now implement one of the following three modifications and report the model's performance.\n",
    "- Change the number of LSTM layers\n",
    "- Change the number of hidden dimensions\n",
    "- Change the number of word embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharPOSTagger_new(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim, \n",
    "                 char_hidden_dim, char_size, vocab_size, tagset_size):\n",
    "        super(CharPOSTagger_new, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # an char level LSTM: that finds the character level embedding for a word\n",
    "        # an LSTM layer: that takes the combined embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "        self.lstm = nn.LSTM(char_hidden_dim + embedding_dim, hidden_dim, \n",
    "                            num_layers=LSTM_LAYERS,\n",
    "                            dropout = DROPOUT if LSTM_LAYERS > 1 else 0)  \n",
    "        \n",
    "        self.embedding_c = nn.Embedding(char_size + 1, char_embedding_dim, padding_idx=1) #char_size + 1, char_embedding_dim\n",
    "        self.lstm_c = nn.LSTM(162, char_hidden_dim,num_layers=LSTM_LAYERS,dropout = DROPOUT if LSTM_LAYERS > 1 else 0)  \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence, chars):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence and a character sequence as the arguments, \n",
    "        # find the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        embedded = self.embedding(sentence)\n",
    "        \n",
    "#         chars = torch.LongTensor([char.numpy() for char in chars])\n",
    "        chars = torch.cat(chars).view(len(chars), 1, -1)\n",
    "    \n",
    "        embedded_c = self.embedding_c(chars)\n",
    "        embedded_c = embedded_c.view(embedded_c.shape[0], 1, -1)\n",
    "#         print('chars', chars.shape)\n",
    "        outputs_c, _ = self.lstm_c(embedded_c)\n",
    "#         word_char_embeddings = torch.cat((word_char_embeddings, outputs_c[-1]), dim = 0)\n",
    "#         print(outputs_c.shape, embedded.shape)\n",
    "            \n",
    "#         print('word_char_embeddings', word_char_embeddings.shape, embedded.shape)\n",
    "        joint_emb = torch.cat((embedded, outputs_c.reshape(outputs_c.shape[0], -1)), dim=1)\n",
    "        joint_emb = joint_emb.reshape(joint_emb.shape[0], 1, -1)\n",
    "        \n",
    "        outputs, _ = self.lstm(joint_emb)\n",
    "        \n",
    "        tag_scores = self.fc(outputs.squeeze(0))\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores\n",
    "\n",
    "def train_char(epoch, model, loss_function, optimizer):\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences as well as character sequences. Find the gradient with \n",
    "        # respect to the loss and update the model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        tags = prepare_sequence(tags, tag_to_idx)\n",
    "        \n",
    "        sentence_chars = []\n",
    "        for w in sentence:\n",
    "            spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "            sentence_chars.append(list(spaces + w))\n",
    "        chars = [prepare_sequence(sentence_char, char_to_idx) for sentence_char in sentence_chars]\n",
    "        \n",
    "        sentence = prepare_sequence(sentence, word_to_idx)\n",
    "        tag_scores = model(sentence, chars)\n",
    "        \n",
    "        tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "        \n",
    "        \n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        model.zero_grad()\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_examples += len(sentence)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate_char(model, loss_function, optimizer)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate_char(model, loss_function, optimizer):\n",
    "    # returns:: avg_val_loss (float)\n",
    "    # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            sentence_chars = []\n",
    "            for w in sentence:\n",
    "                spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "                sentence_chars.append(list(spaces + w))\n",
    "            chars = [prepare_sequence(sentence_char, char_to_idx) for sentence_char in sentence_chars]\n",
    "        \n",
    "            tag_scores = model(prepare_sequence(sentence, word_to_idx), chars)\n",
    "            tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "            tags = prepare_sequence(tags, tag_to_idx)\n",
    "            \n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            val_loss += loss\n",
    "            \n",
    "            max_preds = tag_scores.argmax(dim = 1, keepdim = True).reshape(1,-1) # get the index of the max probability\n",
    "            tags = tags.reshape(1, -1)\n",
    "            correct += int(torch.sum(max_preds==tags))\n",
    "            val_examples += max_preds.shape[1] \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number of LSTM layers\n",
    "# Change the number of hidden dimensions\n",
    "# Change the number of word embedding dimensions\n",
    "# 89.99%\n",
    "# EMBEDDING_DIM = 200\n",
    "# HIDDEN_DIM = 32\n",
    "# LEARNING_RATE = 0.01\n",
    "# LSTM_LAYERS = 2\n",
    "# DROPOUT = 0.1\n",
    "# EPOCHS = 30\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 0.01\n",
    "LSTM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30\tAvg Train Loss: 0.1308\tAvg Val Loss: 0.1263\t Val Accuracy: 15\n",
      "Epoch: 2/30\tAvg Train Loss: 0.1249\tAvg Val Loss: 0.1196\t Val Accuracy: 23\n",
      "Epoch: 3/30\tAvg Train Loss: 0.1046\tAvg Val Loss: 0.0913\t Val Accuracy: 40\n",
      "Epoch: 4/30\tAvg Train Loss: 0.0809\tAvg Val Loss: 0.0727\t Val Accuracy: 52\n",
      "Epoch: 5/30\tAvg Train Loss: 0.0660\tAvg Val Loss: 0.0609\t Val Accuracy: 59\n",
      "Epoch: 6/30\tAvg Train Loss: 0.0556\tAvg Val Loss: 0.0516\t Val Accuracy: 66\n",
      "Epoch: 7/30\tAvg Train Loss: 0.0469\tAvg Val Loss: 0.0438\t Val Accuracy: 71\n",
      "Epoch: 8/30\tAvg Train Loss: 0.0396\tAvg Val Loss: 0.0376\t Val Accuracy: 76\n",
      "Epoch: 9/30\tAvg Train Loss: 0.0334\tAvg Val Loss: 0.0329\t Val Accuracy: 79\n",
      "Epoch: 10/30\tAvg Train Loss: 0.0284\tAvg Val Loss: 0.0288\t Val Accuracy: 82\n",
      "Epoch: 11/30\tAvg Train Loss: 0.0245\tAvg Val Loss: 0.0260\t Val Accuracy: 84\n",
      "Epoch: 12/30\tAvg Train Loss: 0.0214\tAvg Val Loss: 0.0239\t Val Accuracy: 85\n",
      "Epoch: 13/30\tAvg Train Loss: 0.0189\tAvg Val Loss: 0.0223\t Val Accuracy: 87\n",
      "Epoch: 14/30\tAvg Train Loss: 0.0168\tAvg Val Loss: 0.0209\t Val Accuracy: 88\n",
      "Epoch: 15/30\tAvg Train Loss: 0.0151\tAvg Val Loss: 0.0199\t Val Accuracy: 88\n",
      "Epoch: 16/30\tAvg Train Loss: 0.0136\tAvg Val Loss: 0.0191\t Val Accuracy: 89\n",
      "Epoch: 17/30\tAvg Train Loss: 0.0124\tAvg Val Loss: 0.0183\t Val Accuracy: 90\n",
      "Epoch: 18/30\tAvg Train Loss: 0.0112\tAvg Val Loss: 0.0176\t Val Accuracy: 90\n",
      "Epoch: 19/30\tAvg Train Loss: 0.0103\tAvg Val Loss: 0.0173\t Val Accuracy: 90\n",
      "Epoch: 20/30\tAvg Train Loss: 0.0095\tAvg Val Loss: 0.0168\t Val Accuracy: 91\n",
      "Epoch: 21/30\tAvg Train Loss: 0.0087\tAvg Val Loss: 0.0164\t Val Accuracy: 91\n",
      "Epoch: 22/30\tAvg Train Loss: 0.0081\tAvg Val Loss: 0.0161\t Val Accuracy: 91\n",
      "Epoch: 23/30\tAvg Train Loss: 0.0075\tAvg Val Loss: 0.0158\t Val Accuracy: 92\n",
      "Epoch: 24/30\tAvg Train Loss: 0.0070\tAvg Val Loss: 0.0158\t Val Accuracy: 92\n",
      "Epoch: 25/30\tAvg Train Loss: 0.0065\tAvg Val Loss: 0.0156\t Val Accuracy: 92\n",
      "Epoch: 26/30\tAvg Train Loss: 0.0061\tAvg Val Loss: 0.0153\t Val Accuracy: 92\n",
      "Epoch: 27/30\tAvg Train Loss: 0.0057\tAvg Val Loss: 0.0154\t Val Accuracy: 92\n",
      "Epoch: 28/30\tAvg Train Loss: 0.0054\tAvg Val Loss: 0.0153\t Val Accuracy: 92\n",
      "Epoch: 29/30\tAvg Train Loss: 0.0051\tAvg Val Loss: 0.0153\t Val Accuracy: 92\n",
      "Epoch: 30/30\tAvg Train Loss: 0.0049\tAvg Val Loss: 0.0153\t Val Accuracy: 92\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "model_new = CharPOSTagger_new(EMBEDDING_DIM, HIDDEN_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, char_to_idx[' '], VOCAB_SIZE, TAGSET_SIZE)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "        \n",
    "model_new.apply(init_weights)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model_new = model_new.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "optimizer = optim.SGD(model_new.parameters(), lr = LEARNING_RATE)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = loss_function.to(device)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train_char(epoch, model_new, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification choice\n",
    "Which modification did you use and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new():\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    predicted_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentence in test_sentences:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the test loop\n",
    "            # This method saves the predicted tags for the sentences in the test set.\n",
    "            # The tags are first added to a list which is then written to a file for\n",
    "            # submission. An empty string is added after every sequence of tags\n",
    "            # corresponding to a sentence to add a newline following file formatting\n",
    "            # convention, as has been done already.\n",
    "            ############################################################################        \n",
    "            sentence_chars = []\n",
    "            for w in sentence:\n",
    "                spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "                sentence_chars.append(list(spaces + w))\n",
    "            chars = [prepare_sequence(sentence_char, char_to_idx) for sentence_char in sentence_chars]\n",
    "            tag_scores = model_new(prepare_sequence(sentence, word_to_idx), chars)\n",
    "            tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "            max_preds = tag_scores.argmax(dim = 1, keepdim = True)\n",
    "            for tag in max_preds:\n",
    "                tag = int(tag)\n",
    "                predicted_tags.append(idx_to_tag[tag])           \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            predicted_tags.append(\"\")\n",
    "\n",
    "    with open('test_labels.txt', 'w+') as f:\n",
    "        for item in predicted_tags:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "test_new()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6NwhSXaBCNl"
   },
   "source": [
    "### Test accuracy\n",
    "Also evaluate your performance on the test data by submitting test_labels.txt and **report your test accuracy here**.\n",
    "\n",
    "**90.66%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xl2C26leFihB"
   },
   "source": [
    "### Error analysis\n",
    "**Report your findings here.**  \n",
    "Compare the top-10 errors made by this modified model with the errors made by the model from part (a). \n",
    "What errors does the original model make as compared to the modified model, and why do you think it made them? \n",
    "\n",
    "Feel free to reuse the methods defined above for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kh0S5yXIA_0I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_tag\t| model_tag\t| freq.\t| examples\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| JJ\t\t| 218\t| ['drinking', 'worth', 'principal', 'contempt', 'DEPOSIT']\n",
      "-------------------------------------------------------------------------\n",
      "NNP\t| NN\t\t| 170\t| ['Wharton', 'Quantum', 'Schumacher', 'Concorde', 'Elvekrog']\n",
      "-------------------------------------------------------------------------\n",
      "NNP\t| JJ\t\t| 151\t| ['WAVE', 'Aid', 'Works', 'Bessemer', 'Allied-Lyons']\n",
      "-------------------------------------------------------------------------\n",
      "JJ\t| NN\t\t| 142\t| ['desperate', 'ever-narrowing', 'thermal', 'self-indulgent', '2-to-1']\n",
      "-------------------------------------------------------------------------\n",
      "JJ\t| NNP\t\t| 103\t| ['fragmented', 'terse', 'London', 'preferred-stock', 'East']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| NNP\t\t| 98\t| ['veteran', 'Silver', 'introduction', 'mid-1992', 'Power']\n",
      "-------------------------------------------------------------------------\n",
      "VBN\t| VBD\t\t| 76\t| ['released', 'made', 'represented', 'insured', 'Inspired']\n",
      "-------------------------------------------------------------------------\n",
      "NNP\t| NNS\t\t| 75\t| ['Ron', 'HEI', 'Investors', 'Solidarity', 'Door']\n",
      "-------------------------------------------------------------------------\n",
      "IN\t| WDT\t\t| 67\t| ['that', 'that', 'that', 'that', 'that']\n",
      "-------------------------------------------------------------------------\n",
      "NNS\t| NN\t\t| 65\t| ['defaults', 'outages', \"'90s\", 'leaks', 'outskirts']\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions from val data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions_char(model, test_sentences):\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    # Your code here\n",
    "    word_list, model_tags, gt_tags = [], [], []\n",
    "    for sentence, tags in test_sentences:\n",
    "        sentence_chars = []\n",
    "        for w in sentence:\n",
    "            spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "            sentence_chars.append(list(spaces + w))\n",
    "        chars = [prepare_sequence(sentence_char, char_to_idx) for sentence_char in sentence_chars]\n",
    "        tag_scores = model(prepare_sequence(sentence, word_to_idx), chars)\n",
    "        tag_scores = tag_scores.view(-1, tag_scores.shape[-1])\n",
    "\n",
    "        max_preds = tag_scores.argmax(dim = 1, keepdim = True).reshape(1,-1) # get the index of the max probability\n",
    "        model_tags += [idx_to_tag[idx] for idx in max_preds.tolist()[0]]   \n",
    "        word_list += sentence\n",
    "        gt_tags += tags\n",
    "    \n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis_char(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    # Your code here\n",
    "    error_dict = dict()\n",
    "    for idx, word in enumerate(word_list):\n",
    "        if model_tags[idx] != gt_tags[idx]:\n",
    "            tuple_error = (model_tags[idx],gt_tags[idx])\n",
    "            if tuple_error in error_dict:\n",
    "                error_dict[tuple_error] += [word]\n",
    "            else:\n",
    "                error_dict[tuple_error] = [word]\n",
    "    \n",
    "    return sorted(error_dict.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "word_list, model_tags, gt_tags = generate_predictions_char(model_new, val_data)\n",
    "print(\"gt_tag\\t| model_tag\\t| freq.\\t| examples\")\n",
    "horizontal_line = \"-------------------------------------------------------------------------\"\n",
    "errors = error_analysis_char(word_list, model_tags, gt_tags)[:10]\n",
    "print(horizontal_line)\n",
    "for error in errors:\n",
    "    tags, examples = error\n",
    "    model_tag, gt_tag = tags\n",
    "    num_errors, example_words = len(examples), examples\n",
    "    print(\"{}\\t| {}\\t\\t| {}\\t| {}\\n{}\".format(gt_tag, model_tag, num_errors, random.sample(example_words, 5), horizontal_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose the model with 92% validation accuracy with an embedding dimension of 256 and hidden dimension of 64. The test accuracy is 90.66%.\n",
    "\n",
    "The accuracy is improved from 88% to 90%\n",
    "The error between noun is obviously less because we consider the word's structure and sentence's structure at the same time.\n",
    "\n",
    "The same types of errors persist in this model but almost all kinds of error is less.\n",
    "\n",
    "But in part(a), types of verbs is a big problem for the model, but here, only one error include verb. \n",
    "It might be because we consider the word info and char info with a good balance to tag verbs\n",
    "\n",
    "NN | JJ error is higher because now we not only consider the content and the word location, but also consider the word itself. For example, \"drinking\" maybe considered as NN in part(a), when we only cosider its location. But now when we consider the POS of this word (drinking), we need to balance two properties, its content and the word's POS without content. That's why we made more mistake here."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "POS_tagging.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
